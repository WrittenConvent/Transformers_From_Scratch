{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class ConstantLengthDataset(IterableDataset):\n",
    "    def __init__(self, tokenizer, dataset, seq_length=1024,\n",
    "                 num_of_sequences=1024, chars_per_token=3.6):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    m = f\"Buffer Full : {buffer_len}>={self.input_characters:.0f}\"\n",
    "                    # print(m)\n",
    "                    break\n",
    "                try:\n",
    "                    m = f\"Fill Buffer: {buffer_len}<{self.input_characters:.0f}\"\n",
    "                    # print(m)\n",
    "                    buffer.append(next(iterator)[\"content\"])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    iterator = iter(self.dataset)\n",
    "\n",
    "            all_token_ids = []\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)\n",
    "            for tokenized_input in tokenized_inputs[\"input_ids\"]:\n",
    "                all_token_ids.extend(tokenized_input+[self.concat_token_id])\n",
    "\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i+self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    yield torch.tensor(input_ids)\n",
    "\n",
    "def eval_ds_fun(eval_ds):\n",
    "    return {\"eval_input_ids\" : tokenizer(eval_ds[\"content\"], truncation=True, max_length=1024,\n",
    "                      padding=\"max_length\")[\"input_ids\"]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# Commented parameters correspond to the small model\n",
    "config = {\"epochs\": 1,\n",
    "          \"train_batch_size\": 2, # 12\n",
    "          \"valid_batch_size\": 8, # 12\n",
    "          \"weight_decay\": 0.1,\n",
    "          \"shuffle_buffer\": 1000,\n",
    "          \"learning_rate\": 2e-4, # 5e-4\n",
    "          \"lr_scheduler_type\": \"cosine\",\n",
    "          \"num_warmup_steps\": 750, # 2000\n",
    "          \"gradient_accumulation_steps\": 16, # 1\n",
    "          \"max_train_steps\": 20, # 150000\n",
    "          \"max_eval_steps\": -1,\n",
    "          \"seq_length\": 1024,\n",
    "          \"seed\": 1,\n",
    "          \"save_checkpoint_steps\": 50000} # 15000\n",
    "\n",
    "args = Namespace(**config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=1, train_batch_size=2, valid_batch_size=8, weight_decay=0.1, shuffle_buffer=1000, learning_rate=0.0002, lr_scheduler_type='cosine', num_warmup_steps=750, gradient_accumulation_steps=16, max_train_steps=20, max_eval_steps=-1, seq_length=1024, seed=1, save_checkpoint_steps=50000)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-14 18:05:36.732230: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-14 18:05:36.957494: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-14 18:05:37.402349: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-14 18:05:37.402400: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-14 18:05:37.402405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import logging\n",
    "import datasets\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "def setup_logging(project_name):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO, handlers=[\n",
    "                    logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\n",
    "                    logging.StreamHandler()]\n",
    "    )\n",
    "    if accelerator.is_main_process: # We only want to set up logging once\n",
    "        wandb.init(project=project_name, config=args, name=\"this_is\", id=\"random\",\n",
    "                   resume=\"auto\")\n",
    "        run_name = wandb.run.name\n",
    "        tb_writer = SummaryWriter()\n",
    "        tb_writer.add_hparams(vars(args), {'0': 0})\n",
    "        logger.setLevel(logging.INFO)\n",
    "        datasets.utils.logging.set_verbosity_debug()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        tb_writer = None\n",
    "        run_name = ''\n",
    "        logger.setLevel(logging.ERROR)\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    return logger, tb_writer, run_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "def create_dataloaders(dataset_name):\n",
    "    train_data = load_dataset(dataset_name+'-train', split=\"train\",\n",
    "                              streaming=True)\n",
    "    train_data = train_data.shuffle(buffer_size=args.shuffle_buffer,\n",
    "                                    seed=args.seed)\n",
    "    train_dataset = ConstantLengthDataset(tokenizer, train_data,\n",
    "                                          seq_length=args.seq_length)\n",
    "    train_dataloader=DataLoader(train_dataset, batch_size=args.train_batch_size)\n",
    "\n",
    "\n",
    "    valid_data = load_dataset(dataset_name+'-valid', split=\"validation\",\n",
    "                              streaming=False)\n",
    "    valid_data = valid_data.map(eval_ds_fun, batched=True, batch_size=128,\n",
    "                    remove_columns=['repo_name', 'path', 'copies', 'size', 'content', 'license'])\n",
    "    eval_dataloader = DataLoader(torch.tensor(valid_data[\"eval_input_ids\"][:200]), batch_size=8)\n",
    "    #### remove [:200] from above!\n",
    "\n",
    "    return train_dataloader, eval_dataloader\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "\n",
    "    return [{\"params\":params_with_wd, \"weight_decay\":args.weight_decay},\n",
    "            {\"params\":params_without_wd, \"weight_decay\":0.0}]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33msusnato\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668484783319098, max=1.0â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6310df53803a407185c94a33a439141c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.13.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.7"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/susnato/PycharmProjects/Transformers_From_Scratch/wandb/run-20230114_180539-random</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Resuming run <strong><a href=\"https://wandb.ai/susnato/codeparrot-small/runs/random\" target=\"_blank\">this_is</a></strong> to <a href=\"https://wandb.ai/susnato/codeparrot-small\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/14/2023 18:05:40 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "Mixed precision type: no\n",
      "\n",
      "loading configuration file config.json from cache at /home/susnato/.cache/huggingface/hub/models--susnato--codeparrot-small2/snapshots/98c2bd764fddd5d81402fb3bf55ae51454b4f639/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"susnato/codeparrot-small2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/457M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fa1ab2403b048c297a07796ab8b8b58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /home/susnato/.cache/huggingface/hub/models--susnato--codeparrot-small2/snapshots/98c2bd764fddd5d81402fb3bf55ae51454b4f639/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at susnato/codeparrot-small2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading file vocab.json from cache at /home/susnato/.cache/huggingface/hub/models--susnato--codeparrot/snapshots/58e55c7b745f851839fd00898005d2415aa4e975/vocab.json\n",
      "loading file merges.txt from cache at /home/susnato/.cache/huggingface/hub/models--susnato--codeparrot/snapshots/58e55c7b745f851839fd00898005d2415aa4e975/merges.txt\n",
      "loading file tokenizer.json from cache at /home/susnato/.cache/huggingface/hub/models--susnato--codeparrot/snapshots/58e55c7b745f851839fd00898005d2415aa4e975/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/susnato/.cache/huggingface/hub/models--susnato--codeparrot/snapshots/58e55c7b745f851839fd00898005d2415aa4e975/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/susnato/.cache/huggingface/hub/models--susnato--codeparrot/snapshots/58e55c7b745f851839fd00898005d2415aa4e975/tokenizer_config.json\n",
      "Assigning <|endoftext|> to the pad_token key of the tokenizer\n",
      "01/14/2023 18:06:35 - WARNING - datasets.builder - Using custom data configuration transformersbook--codeparrot-train-ba60c789679753de\n",
      "01/14/2023 18:06:35 - INFO - datasets.info - Loading Dataset Infos from /home/susnato/anaconda3/envs/transformers/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "01/14/2023 18:06:35 - DEBUG - datasets.utils.filelock - Attempting to acquire lock 140118033996816 on /home/susnato/.cache/huggingface/datasets/_home_susnato_.cache_huggingface_datasets_transformersbook___json_transformersbook--codeparrot-train-ba60c789679753de_0.0.0_0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51.lock\n",
      "01/14/2023 18:06:35 - DEBUG - datasets.utils.filelock - Lock 140118033996816 acquired on /home/susnato/.cache/huggingface/datasets/_home_susnato_.cache_huggingface_datasets_transformersbook___json_transformersbook--codeparrot-train-ba60c789679753de_0.0.0_0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51.lock\n",
      "01/14/2023 18:06:35 - DEBUG - datasets.utils.filelock - Attempting to release lock 140118033996816 on /home/susnato/.cache/huggingface/datasets/_home_susnato_.cache_huggingface_datasets_transformersbook___json_transformersbook--codeparrot-train-ba60c789679753de_0.0.0_0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51.lock\n",
      "01/14/2023 18:06:35 - DEBUG - datasets.utils.filelock - Lock 140118033996816 released on /home/susnato/.cache/huggingface/datasets/_home_susnato_.cache_huggingface_datasets_transformersbook___json_transformersbook--codeparrot-train-ba60c789679753de_0.0.0_0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51.lock\n",
      "01/14/2023 18:06:40 - WARNING - datasets.builder - Using custom data configuration transformersbook--codeparrot-valid-716632e8b375a328\n",
      "01/14/2023 18:06:40 - INFO - datasets.info - Loading Dataset Infos from /home/susnato/anaconda3/envs/transformers/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "01/14/2023 18:06:40 - DEBUG - datasets.utils.filelock - Attempting to acquire lock 140118025796864 on /home/susnato/.cache/huggingface/datasets/_home_susnato_.cache_huggingface_datasets_transformersbook___json_transformersbook--codeparrot-valid-716632e8b375a328_0.0.0_0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51.lock\n",
      "01/14/2023 18:06:40 - DEBUG - datasets.utils.filelock - Lock 140118025796864 acquired on /home/susnato/.cache/huggingface/datasets/_home_susnato_.cache_huggingface_datasets_transformersbook___json_transformersbook--codeparrot-valid-716632e8b375a328_0.0.0_0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51.lock\n",
      "01/14/2023 18:06:40 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "01/14/2023 18:06:40 - INFO - datasets.info - Loading Dataset info from /home/susnato/.cache/huggingface/datasets/transformersbook___json/transformersbook--codeparrot-valid-716632e8b375a328/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
      "01/14/2023 18:06:40 - DEBUG - datasets.utils.filelock - Attempting to release lock 140118025796864 on /home/susnato/.cache/huggingface/datasets/_home_susnato_.cache_huggingface_datasets_transformersbook___json_transformersbook--codeparrot-valid-716632e8b375a328_0.0.0_0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51.lock\n",
      "01/14/2023 18:06:40 - DEBUG - datasets.utils.filelock - Lock 140118025796864 released on /home/susnato/.cache/huggingface/datasets/_home_susnato_.cache_huggingface_datasets_transformersbook___json_transformersbook--codeparrot-valid-716632e8b375a328_0.0.0_0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51.lock\n",
      "01/14/2023 18:06:40 - DEBUG - datasets.utils.filelock - Attempting to acquire lock 140118025796672 on /home/susnato/.cache/huggingface/datasets/transformersbook___json/transformersbook--codeparrot-valid-716632e8b375a328/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51_builder.lock\n",
      "01/14/2023 18:06:40 - DEBUG - datasets.utils.filelock - Lock 140118025796672 acquired on /home/susnato/.cache/huggingface/datasets/transformersbook___json/transformersbook--codeparrot-valid-716632e8b375a328/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51_builder.lock\n",
      "01/14/2023 18:06:40 - WARNING - datasets.builder - Found cached dataset json (/home/susnato/.cache/huggingface/datasets/transformersbook___json/transformersbook--codeparrot-valid-716632e8b375a328/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "01/14/2023 18:06:40 - INFO - datasets.info - Loading Dataset info from /home/susnato/.cache/huggingface/datasets/transformersbook___json/transformersbook--codeparrot-valid-716632e8b375a328/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
      "01/14/2023 18:06:40 - DEBUG - datasets.utils.filelock - Attempting to release lock 140118025796672 on /home/susnato/.cache/huggingface/datasets/transformersbook___json/transformersbook--codeparrot-valid-716632e8b375a328/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51_builder.lock\n",
      "01/14/2023 18:06:40 - DEBUG - datasets.utils.filelock - Lock 140118025796672 released on /home/susnato/.cache/huggingface/datasets/transformersbook___json/transformersbook--codeparrot-valid-716632e8b375a328/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51_builder.lock\n",
      "01/14/2023 18:06:40 - DEBUG - datasets.builder - Constructing Dataset for split validation, from /home/susnato/.cache/huggingface/datasets/transformersbook___json/transformersbook--codeparrot-valid-716632e8b375a328/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
      "01/14/2023 18:06:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/susnato/.cache/huggingface/datasets/transformersbook___json/transformersbook--codeparrot-valid-716632e8b375a328/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-28cc22362db0c4b2.arrow\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from transformers import set_seed\n",
    "from transformers import get_scheduler\n",
    "from huggingface_hub import Repository\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "set_seed(args.seed)\n",
    "project_name = \"codeparrot-small\"\n",
    "model_to_be_trained = \"codeparrot-small2\"\n",
    "\n",
    "samples_per_step = accelerator.state.num_processes * args.train_batch_size\n",
    "\n",
    "#Logging\n",
    "logger, tb_writer, run_name = setup_logging(project_name)\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "#Load model and tokenizer\n",
    "# if accelerator.is_main_process:\n",
    "#     hf_repo = Repository(\"./Training_files/git_files/\",\n",
    "#                          clone_from=\"susnato/codeparrot-training-from-scratch\")\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"susnato/{model_to_be_trained}\")\n",
    "model.gradient_checkpointing_enable()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"susnato/codeparrot\")\n",
    "tokenizer.add_special_tokens({'pad_token' : '<|endoftext|>'})\n",
    "\n",
    "#Load Dataset and DataLoader\n",
    "train_dl, eval_dl = create_dataloaders(\"transformersbook/codeparrot\")\n",
    "\n",
    "#Prepare the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\n",
    "lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n",
    "                          num_warmup_steps=args.num_warmup_steps,\n",
    "                          num_training_steps=args.max_train_steps)\n",
    "\n",
    "def get_lr():\n",
    "    return optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "model, optimizer, train_dl, eval_dl = accelerator.prepare(model, optimizer, train_dl, eval_dl)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dl):\n",
    "        with torch.no_grad():\n",
    "            opts = model(batch, labels=batch)\n",
    "            loss = opts.loss.repeat(args.valid_batch_size)\n",
    "            losses.append(accelerator.gather(loss))\n",
    "            if args.max_eval_steps > 0 and step >= args.max_eval_steps:\n",
    "                break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()\n",
    "\n",
    "def log_metrics(epoch, step, metrics):\n",
    "    metrics.update({\"Epoch\":epoch})\n",
    "    logger.info(f\"Step {step} : {metrics}\")\n",
    "    if accelerator.is_main_process:\n",
    "        wandb.log(metrics)\n",
    "        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "#Train Model\n",
    "model.train()\n",
    "\n",
    "def run_epochs(epochs):\n",
    "    if os.path.isfile(\"/home/susnato/PycharmProjects/Transformers_From_Scratch\"\n",
    "                             f\"/Training_files/epochs_steps.pickle\"):\n",
    "        epoch_step_file = pickle.load(\n",
    "            open(\"/home/susnato/PycharmProjects/Transformers_From_Scratch\"\n",
    "                             f\"/Training_files/epochs_steps.pickle\", \"rb\"))\n",
    "        completed_steps = epoch_step_file[\"completed_steps\"]\n",
    "        if completed_steps==0:\n",
    "            curr_epoch = epoch_step_file[\"curr_epoch\"]\n",
    "        else:\n",
    "            curr_epoch = epoch_step_file[\"curr_epoch\"]-1\n",
    "        print(f\"Prev epoch/steps File found !, got :: epoch - {curr_epoch} steps -\"\n",
    "              f\" {completed_steps}\")\n",
    "    else:\n",
    "        curr_epoch = 0\n",
    "        completed_steps = 0\n",
    "    for epoch in range(1+curr_epoch, curr_epoch+epochs+1):\n",
    "        print(f\"\"\"####################################################################################\n",
    "                                    Starting Epoch - {epoch}\n",
    "            ####################################################################################\n",
    "                \"\"\")\n",
    "        for step, batch in enumerate(train_dl, start=1):\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "            loss = model(batch, labels=batch).loss\n",
    "            log_metrics(epoch, step, {\"lr\":get_lr(), \"samples\":step*samples_per_step,\n",
    "                               \"steps\":completed_steps, \"loss/train\":loss.item()})\n",
    "            loss = loss/args.gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            if step % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                completed_steps += 1\n",
    "            if step % args.save_checkpoint_steps == 0:\n",
    "                logger.info('Evaluating and saving model checkpoint')\n",
    "                eval_loss, perplexity = evaluate()\n",
    "                log_metrics(epoch, step, {\"loss/eval\":eval_loss, \"perplexity\":perplexity})\n",
    "                accelerator.wait_for_everyone()\n",
    "                unwrapped_model = accelerator.unwrap_model(model)\n",
    "\n",
    "                unwrapped_model.save_pretrained(f\"Training_files/models/{model_to_be_trained}\")\n",
    "                unwrapped_model.push_to_hub(repo_id=\"susnato/codeparrot-small2\",\n",
    "                                    commit_message=f\"Trained for Epoch - {epoch}, save_checkpoint_steps reached!\")\n",
    "            #Log Epoch No and completed_steps\n",
    "            log_dict = {\"curr_epoch\":epoch, \"completed_steps\":completed_steps}\n",
    "            with open(\"/home/susnato/PycharmProjects/Transformers_From_Scratch\"\n",
    "                             f\"/Training_files/epochs_steps.pickle\", \"wb\") as file:\n",
    "                pickle.dump(obj=log_dict, file=file)\n",
    "\n",
    "\n",
    "        logger.info(\"Evaluating and Saving model after training\")\n",
    "\n",
    "        #Saving\n",
    "        completed_steps = 0\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(f\"Training_files/models/{model_to_be_trained}\")\n",
    "        unwrapped_model.push_to_hub(repo_id=\"susnato/codeparrot-small2\",\n",
    "                                    commit_message=f\"Trained for Epoch - {epoch}\")\n",
    "\n",
    "        eval_loss, perplexity = evaluate()\n",
    "        log_metrics(epoch, step, {\"loss/eval\":eval_loss, \"perplexity\":perplexity})\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "        log_dict = {\"curr_epoch\":epoch, \"completed_steps\":completed_steps}\n",
    "        with open(\"/home/susnato/PycharmProjects/Transformers_From_Scratch\"\n",
    "                             f\"/Training_files/epochs_steps.pickle\", \"wb\") as file:\n",
    "            pickle.dump(obj=log_dict, file=file)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You must be root to use this library on linux.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeyboard\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mkeyboard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_pressed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43ma\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou pressed \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.9/site-packages/keyboard/__init__.py:410\u001B[0m, in \u001B[0;36mis_pressed\u001B[0;34m(hotkey)\u001B[0m\n\u001B[1;32m    402\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_pressed\u001B[39m(hotkey):\n\u001B[1;32m    403\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;124;03m    Returns True if the key is pressed.\u001B[39;00m\n\u001B[1;32m    405\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;124;03m        is_pressed('ctrl+space') #-> True\u001B[39;00m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 410\u001B[0m     \u001B[43m_listener\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_if_necessary\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    412\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_number(hotkey):\n\u001B[1;32m    413\u001B[0m         \u001B[38;5;66;03m# Shortcut.\u001B[39;00m\n\u001B[1;32m    414\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m _pressed_events_lock:\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.9/site-packages/keyboard/_generic.py:35\u001B[0m, in \u001B[0;36mGenericListener.start_if_necessary\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlistening:\n\u001B[0;32m---> 35\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlistening \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     38\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlistening_thread \u001B[38;5;241m=\u001B[39m Thread(target\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlisten)\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.9/site-packages/keyboard/__init__.py:196\u001B[0m, in \u001B[0;36m_KeyboardListener.init\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minit\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 196\u001B[0m     \u001B[43m_os_keyboard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_modifiers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocking_hooks \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.9/site-packages/keyboard/_nixkeyboard.py:113\u001B[0m, in \u001B[0;36minit\u001B[0;34m()\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minit\u001B[39m():\n\u001B[0;32m--> 113\u001B[0m     \u001B[43mbuild_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m     build_tables()\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.9/site-packages/keyboard/_nixkeyboard.py:109\u001B[0m, in \u001B[0;36mbuild_device\u001B[0;34m()\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m device\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device: \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 109\u001B[0m \u001B[43mensure_root\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    110\u001B[0m device \u001B[38;5;241m=\u001B[39m aggregate_devices(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkbd\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.9/site-packages/keyboard/_nixcommon.py:174\u001B[0m, in \u001B[0;36mensure_root\u001B[0;34m()\u001B[0m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mensure_root\u001B[39m():\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mgeteuid() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 174\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYou must be root to use this library on linux.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mImportError\u001B[0m: You must be root to use this library on linux."
     ]
    }
   ],
   "source": [
    "import keyboard\n",
    "while True:\n",
    "    if keyboard.is_pressed(\"a\"):\n",
    "        print(\"You pressed 'a'.\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "while True:\n",
    "    if ord(\"q\"):\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prev epoch/steps File found !, got :: epoch - 7 steps - 20\n",
      "####################################################################################\n",
      "                                    Starting Epoch - 8\n",
      "            ####################################################################################\n",
      "                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2626 > 1024). Running this sequence through the model will result in indexing errors\n",
      "01/14/2023 18:07:15 - INFO - __main__ - Evaluating and Saving model after training\n",
      "01/14/2023 18:07:27 - INFO - __main__ - Step 1 : {'loss/eval': 7.925716876983643, 'perplexity': 2767.547607421875, 'Epoch': 8}\n",
      "Configuration saved in Training_files/models/codeparrot-small2/config.json\n",
      "Model weights saved in Training_files/models/codeparrot-small2/pytorch_model.bin\n",
      "Configuration saved in /tmp/tmpmq36wvbc/config.json\n",
      "Model weights saved in /tmp/tmpmq36wvbc/pytorch_model.bin\n",
      "Uploading the following files to susnato/codeparrot-small2: config.json,pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################\n",
      "                                    Starting Epoch - 9\n",
      "            ####################################################################################\n",
      "                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/14/2023 18:07:37 - INFO - __main__ - Step 1 : {'lr': 0.0, 'samples': 2, 'steps': 0, 'loss/train': 7.354959964752197, 'Epoch': 9}\n",
      "01/14/2023 18:07:37 - INFO - __main__ - Step 2 : {'lr': 0.0, 'samples': 4, 'steps': 0, 'loss/train': 7.419513702392578, 'Epoch': 9}\n",
      "01/14/2023 18:07:38 - INFO - __main__ - Step 3 : {'lr': 0.0, 'samples': 6, 'steps': 0, 'loss/train': 7.648837089538574, 'Epoch': 9}\n",
      "01/14/2023 18:07:38 - INFO - __main__ - Step 4 : {'lr': 0.0, 'samples': 8, 'steps': 0, 'loss/train': 7.377267837524414, 'Epoch': 9}\n",
      "01/14/2023 18:07:38 - INFO - __main__ - Step 5 : {'lr': 0.0, 'samples': 10, 'steps': 0, 'loss/train': 7.237504959106445, 'Epoch': 9}\n",
      "01/14/2023 18:07:39 - INFO - __main__ - Step 6 : {'lr': 0.0, 'samples': 12, 'steps': 0, 'loss/train': 7.551029205322266, 'Epoch': 9}\n",
      "01/14/2023 18:07:39 - INFO - __main__ - Step 7 : {'lr': 0.0, 'samples': 14, 'steps': 0, 'loss/train': 7.976627349853516, 'Epoch': 9}\n",
      "01/14/2023 18:07:39 - INFO - __main__ - Step 8 : {'lr': 0.0, 'samples': 16, 'steps': 0, 'loss/train': 7.771538257598877, 'Epoch': 9}\n",
      "01/14/2023 18:07:40 - INFO - __main__ - Step 9 : {'lr': 0.0, 'samples': 18, 'steps': 0, 'loss/train': 7.036052703857422, 'Epoch': 9}\n",
      "01/14/2023 18:07:40 - INFO - __main__ - Step 10 : {'lr': 0.0, 'samples': 20, 'steps': 0, 'loss/train': 6.914931774139404, 'Epoch': 9}\n",
      "01/14/2023 18:07:41 - INFO - __main__ - Step 11 : {'lr': 0.0, 'samples': 22, 'steps': 0, 'loss/train': 6.94704532623291, 'Epoch': 9}\n",
      "01/14/2023 18:07:41 - INFO - __main__ - Step 12 : {'lr': 0.0, 'samples': 24, 'steps': 0, 'loss/train': 7.58168888092041, 'Epoch': 9}\n",
      "01/14/2023 18:07:41 - INFO - __main__ - Step 13 : {'lr': 0.0, 'samples': 26, 'steps': 0, 'loss/train': 7.451148986816406, 'Epoch': 9}\n",
      "01/14/2023 18:07:42 - INFO - __main__ - Step 14 : {'lr': 0.0, 'samples': 28, 'steps': 0, 'loss/train': 7.6945695877075195, 'Epoch': 9}\n",
      "01/14/2023 18:07:42 - INFO - __main__ - Step 15 : {'lr': 0.0, 'samples': 30, 'steps': 0, 'loss/train': 7.9531569480896, 'Epoch': 9}\n",
      "01/14/2023 18:07:42 - INFO - __main__ - Step 16 : {'lr': 0.0, 'samples': 32, 'steps': 0, 'loss/train': 7.347620964050293, 'Epoch': 9}\n",
      "01/14/2023 18:07:43 - INFO - __main__ - Step 17 : {'lr': 2.6666666666666667e-07, 'samples': 34, 'steps': 1, 'loss/train': 7.3216166496276855, 'Epoch': 9}\n",
      "01/14/2023 18:07:43 - INFO - __main__ - Step 18 : {'lr': 2.6666666666666667e-07, 'samples': 36, 'steps': 1, 'loss/train': 6.6584272384643555, 'Epoch': 9}\n",
      "01/14/2023 18:07:44 - INFO - __main__ - Step 19 : {'lr': 2.6666666666666667e-07, 'samples': 38, 'steps': 1, 'loss/train': 5.978687763214111, 'Epoch': 9}\n",
      "01/14/2023 18:07:44 - INFO - __main__ - Step 20 : {'lr': 2.6666666666666667e-07, 'samples': 40, 'steps': 1, 'loss/train': 6.951815128326416, 'Epoch': 9}\n",
      "01/14/2023 18:07:44 - INFO - __main__ - Step 21 : {'lr': 2.6666666666666667e-07, 'samples': 42, 'steps': 1, 'loss/train': 7.361059188842773, 'Epoch': 9}\n",
      "01/14/2023 18:07:45 - INFO - __main__ - Step 22 : {'lr': 2.6666666666666667e-07, 'samples': 44, 'steps': 1, 'loss/train': 7.873469352722168, 'Epoch': 9}\n",
      "01/14/2023 18:07:45 - INFO - __main__ - Step 23 : {'lr': 2.6666666666666667e-07, 'samples': 46, 'steps': 1, 'loss/train': 7.812808036804199, 'Epoch': 9}\n",
      "01/14/2023 18:07:46 - INFO - __main__ - Step 24 : {'lr': 2.6666666666666667e-07, 'samples': 48, 'steps': 1, 'loss/train': 8.18108081817627, 'Epoch': 9}\n",
      "01/14/2023 18:07:46 - INFO - __main__ - Step 25 : {'lr': 2.6666666666666667e-07, 'samples': 50, 'steps': 1, 'loss/train': 8.188691139221191, 'Epoch': 9}\n",
      "01/14/2023 18:07:46 - INFO - __main__ - Step 26 : {'lr': 2.6666666666666667e-07, 'samples': 52, 'steps': 1, 'loss/train': 8.211323738098145, 'Epoch': 9}\n",
      "01/14/2023 18:07:47 - INFO - __main__ - Step 27 : {'lr': 2.6666666666666667e-07, 'samples': 54, 'steps': 1, 'loss/train': 7.9393696784973145, 'Epoch': 9}\n",
      "01/14/2023 18:07:47 - INFO - __main__ - Step 28 : {'lr': 2.6666666666666667e-07, 'samples': 56, 'steps': 1, 'loss/train': 7.766664028167725, 'Epoch': 9}\n",
      "01/14/2023 18:07:48 - INFO - __main__ - Step 29 : {'lr': 2.6666666666666667e-07, 'samples': 58, 'steps': 1, 'loss/train': 7.375805854797363, 'Epoch': 9}\n",
      "01/14/2023 18:07:48 - INFO - __main__ - Step 30 : {'lr': 2.6666666666666667e-07, 'samples': 60, 'steps': 1, 'loss/train': 5.637142658233643, 'Epoch': 9}\n",
      "01/14/2023 18:07:48 - INFO - __main__ - Step 31 : {'lr': 2.6666666666666667e-07, 'samples': 62, 'steps': 1, 'loss/train': 5.437601089477539, 'Epoch': 9}\n",
      "01/14/2023 18:07:49 - INFO - __main__ - Step 32 : {'lr': 2.6666666666666667e-07, 'samples': 64, 'steps': 1, 'loss/train': 6.734180450439453, 'Epoch': 9}\n",
      "01/14/2023 18:07:49 - INFO - __main__ - Step 33 : {'lr': 5.333333333333333e-07, 'samples': 66, 'steps': 2, 'loss/train': 8.097902297973633, 'Epoch': 9}\n",
      "01/14/2023 18:07:50 - INFO - __main__ - Step 34 : {'lr': 5.333333333333333e-07, 'samples': 68, 'steps': 2, 'loss/train': 7.358931064605713, 'Epoch': 9}\n",
      "01/14/2023 18:07:50 - INFO - __main__ - Step 35 : {'lr': 5.333333333333333e-07, 'samples': 70, 'steps': 2, 'loss/train': 7.643469333648682, 'Epoch': 9}\n",
      "01/14/2023 18:07:50 - INFO - __main__ - Step 36 : {'lr': 5.333333333333333e-07, 'samples': 72, 'steps': 2, 'loss/train': 6.7566752433776855, 'Epoch': 9}\n",
      "01/14/2023 18:07:51 - INFO - __main__ - Step 37 : {'lr': 5.333333333333333e-07, 'samples': 74, 'steps': 2, 'loss/train': 7.977330207824707, 'Epoch': 9}\n",
      "01/14/2023 18:07:51 - INFO - __main__ - Step 38 : {'lr': 5.333333333333333e-07, 'samples': 76, 'steps': 2, 'loss/train': 8.166949272155762, 'Epoch': 9}\n",
      "01/14/2023 18:07:51 - INFO - __main__ - Step 39 : {'lr': 5.333333333333333e-07, 'samples': 78, 'steps': 2, 'loss/train': 6.942739009857178, 'Epoch': 9}\n",
      "01/14/2023 18:07:52 - INFO - __main__ - Step 40 : {'lr': 5.333333333333333e-07, 'samples': 80, 'steps': 2, 'loss/train': 6.534844875335693, 'Epoch': 9}\n",
      "01/14/2023 18:07:52 - INFO - __main__ - Step 41 : {'lr': 5.333333333333333e-07, 'samples': 82, 'steps': 2, 'loss/train': 6.508447170257568, 'Epoch': 9}\n",
      "01/14/2023 18:07:52 - INFO - __main__ - Step 42 : {'lr': 5.333333333333333e-07, 'samples': 84, 'steps': 2, 'loss/train': 8.158797264099121, 'Epoch': 9}\n",
      "01/14/2023 18:07:53 - INFO - __main__ - Step 43 : {'lr': 5.333333333333333e-07, 'samples': 86, 'steps': 2, 'loss/train': 8.410614013671875, 'Epoch': 9}\n",
      "01/14/2023 18:07:53 - INFO - __main__ - Step 44 : {'lr': 5.333333333333333e-07, 'samples': 88, 'steps': 2, 'loss/train': 8.097412109375, 'Epoch': 9}\n",
      "01/14/2023 18:07:54 - INFO - __main__ - Step 45 : {'lr': 5.333333333333333e-07, 'samples': 90, 'steps': 2, 'loss/train': 7.290360450744629, 'Epoch': 9}\n",
      "01/14/2023 18:07:54 - INFO - __main__ - Step 46 : {'lr': 5.333333333333333e-07, 'samples': 92, 'steps': 2, 'loss/train': 7.2788848876953125, 'Epoch': 9}\n",
      "01/14/2023 18:07:55 - INFO - __main__ - Step 47 : {'lr': 5.333333333333333e-07, 'samples': 94, 'steps': 2, 'loss/train': 7.813963413238525, 'Epoch': 9}\n",
      "01/14/2023 18:07:55 - INFO - __main__ - Step 48 : {'lr': 5.333333333333333e-07, 'samples': 96, 'steps': 2, 'loss/train': 7.325280666351318, 'Epoch': 9}\n",
      "01/14/2023 18:07:55 - INFO - __main__ - Step 49 : {'lr': 8.000000000000001e-07, 'samples': 98, 'steps': 3, 'loss/train': 7.535931587219238, 'Epoch': 9}\n",
      "01/14/2023 18:07:56 - INFO - __main__ - Step 50 : {'lr': 8.000000000000001e-07, 'samples': 100, 'steps': 3, 'loss/train': 7.058873653411865, 'Epoch': 9}\n",
      "01/14/2023 18:07:56 - INFO - __main__ - Step 51 : {'lr': 8.000000000000001e-07, 'samples': 102, 'steps': 3, 'loss/train': 7.350874900817871, 'Epoch': 9}\n",
      "01/14/2023 18:07:56 - INFO - __main__ - Step 52 : {'lr': 8.000000000000001e-07, 'samples': 104, 'steps': 3, 'loss/train': 7.235696792602539, 'Epoch': 9}\n",
      "01/14/2023 18:07:57 - INFO - __main__ - Step 53 : {'lr': 8.000000000000001e-07, 'samples': 106, 'steps': 3, 'loss/train': 7.607357978820801, 'Epoch': 9}\n",
      "01/14/2023 18:07:57 - INFO - __main__ - Step 54 : {'lr': 8.000000000000001e-07, 'samples': 108, 'steps': 3, 'loss/train': 7.768067836761475, 'Epoch': 9}\n",
      "01/14/2023 18:07:58 - INFO - __main__ - Step 55 : {'lr': 8.000000000000001e-07, 'samples': 110, 'steps': 3, 'loss/train': 7.293904781341553, 'Epoch': 9}\n",
      "01/14/2023 18:07:58 - INFO - __main__ - Step 56 : {'lr': 8.000000000000001e-07, 'samples': 112, 'steps': 3, 'loss/train': 7.647892951965332, 'Epoch': 9}\n",
      "01/14/2023 18:07:59 - INFO - __main__ - Step 57 : {'lr': 8.000000000000001e-07, 'samples': 114, 'steps': 3, 'loss/train': 7.579688549041748, 'Epoch': 9}\n",
      "01/14/2023 18:07:59 - INFO - __main__ - Step 58 : {'lr': 8.000000000000001e-07, 'samples': 116, 'steps': 3, 'loss/train': 7.691546440124512, 'Epoch': 9}\n",
      "01/14/2023 18:08:00 - INFO - __main__ - Step 59 : {'lr': 8.000000000000001e-07, 'samples': 118, 'steps': 3, 'loss/train': 8.726272583007812, 'Epoch': 9}\n",
      "01/14/2023 18:08:00 - INFO - __main__ - Step 60 : {'lr': 8.000000000000001e-07, 'samples': 120, 'steps': 3, 'loss/train': 7.603463649749756, 'Epoch': 9}\n",
      "01/14/2023 18:08:00 - INFO - __main__ - Step 61 : {'lr': 8.000000000000001e-07, 'samples': 122, 'steps': 3, 'loss/train': 7.41049861907959, 'Epoch': 9}\n",
      "01/14/2023 18:08:01 - INFO - __main__ - Step 62 : {'lr': 8.000000000000001e-07, 'samples': 124, 'steps': 3, 'loss/train': 6.768327236175537, 'Epoch': 9}\n",
      "01/14/2023 18:08:01 - INFO - __main__ - Step 63 : {'lr': 8.000000000000001e-07, 'samples': 126, 'steps': 3, 'loss/train': 6.996535301208496, 'Epoch': 9}\n",
      "01/14/2023 18:08:02 - INFO - __main__ - Step 64 : {'lr': 8.000000000000001e-07, 'samples': 128, 'steps': 3, 'loss/train': 7.027439594268799, 'Epoch': 9}\n",
      "01/14/2023 18:08:02 - INFO - __main__ - Step 65 : {'lr': 1.0666666666666667e-06, 'samples': 130, 'steps': 4, 'loss/train': 7.498536586761475, 'Epoch': 9}\n",
      "01/14/2023 18:08:02 - INFO - __main__ - Step 66 : {'lr': 1.0666666666666667e-06, 'samples': 132, 'steps': 4, 'loss/train': 7.3374738693237305, 'Epoch': 9}\n",
      "01/14/2023 18:08:03 - INFO - __main__ - Step 67 : {'lr': 1.0666666666666667e-06, 'samples': 134, 'steps': 4, 'loss/train': 7.168976306915283, 'Epoch': 9}\n",
      "01/14/2023 18:08:03 - INFO - __main__ - Step 68 : {'lr': 1.0666666666666667e-06, 'samples': 136, 'steps': 4, 'loss/train': 7.330260276794434, 'Epoch': 9}\n",
      "01/14/2023 18:08:03 - INFO - __main__ - Step 69 : {'lr': 1.0666666666666667e-06, 'samples': 138, 'steps': 4, 'loss/train': 8.312862396240234, 'Epoch': 9}\n",
      "01/14/2023 18:08:04 - INFO - __main__ - Step 70 : {'lr': 1.0666666666666667e-06, 'samples': 140, 'steps': 4, 'loss/train': 7.983168601989746, 'Epoch': 9}\n",
      "01/14/2023 18:08:04 - INFO - __main__ - Step 71 : {'lr': 1.0666666666666667e-06, 'samples': 142, 'steps': 4, 'loss/train': 7.992886066436768, 'Epoch': 9}\n",
      "01/14/2023 18:08:04 - INFO - __main__ - Step 72 : {'lr': 1.0666666666666667e-06, 'samples': 144, 'steps': 4, 'loss/train': 7.8062920570373535, 'Epoch': 9}\n",
      "01/14/2023 18:08:05 - INFO - __main__ - Step 73 : {'lr': 1.0666666666666667e-06, 'samples': 146, 'steps': 4, 'loss/train': 7.394461154937744, 'Epoch': 9}\n",
      "01/14/2023 18:08:05 - INFO - __main__ - Step 74 : {'lr': 1.0666666666666667e-06, 'samples': 148, 'steps': 4, 'loss/train': 7.642642974853516, 'Epoch': 9}\n",
      "01/14/2023 18:08:06 - INFO - __main__ - Step 75 : {'lr': 1.0666666666666667e-06, 'samples': 150, 'steps': 4, 'loss/train': 7.2800822257995605, 'Epoch': 9}\n",
      "01/14/2023 18:08:06 - INFO - __main__ - Step 76 : {'lr': 1.0666666666666667e-06, 'samples': 152, 'steps': 4, 'loss/train': 8.130468368530273, 'Epoch': 9}\n",
      "01/14/2023 18:08:06 - INFO - __main__ - Step 77 : {'lr': 1.0666666666666667e-06, 'samples': 154, 'steps': 4, 'loss/train': 7.8824076652526855, 'Epoch': 9}\n",
      "01/14/2023 18:08:07 - INFO - __main__ - Step 78 : {'lr': 1.0666666666666667e-06, 'samples': 156, 'steps': 4, 'loss/train': 8.159468650817871, 'Epoch': 9}\n",
      "01/14/2023 18:08:07 - INFO - __main__ - Step 79 : {'lr': 1.0666666666666667e-06, 'samples': 158, 'steps': 4, 'loss/train': 7.832133769989014, 'Epoch': 9}\n",
      "01/14/2023 18:08:07 - INFO - __main__ - Step 80 : {'lr': 1.0666666666666667e-06, 'samples': 160, 'steps': 4, 'loss/train': 7.704090595245361, 'Epoch': 9}\n",
      "01/14/2023 18:08:08 - INFO - __main__ - Step 81 : {'lr': 1.3333333333333334e-06, 'samples': 162, 'steps': 5, 'loss/train': 7.725374221801758, 'Epoch': 9}\n",
      "01/14/2023 18:08:08 - INFO - __main__ - Step 82 : {'lr': 1.3333333333333334e-06, 'samples': 164, 'steps': 5, 'loss/train': 7.7044291496276855, 'Epoch': 9}\n",
      "01/14/2023 18:08:09 - INFO - __main__ - Step 83 : {'lr': 1.3333333333333334e-06, 'samples': 166, 'steps': 5, 'loss/train': 7.677286624908447, 'Epoch': 9}\n",
      "01/14/2023 18:08:09 - INFO - __main__ - Step 84 : {'lr': 1.3333333333333334e-06, 'samples': 168, 'steps': 5, 'loss/train': 7.721625328063965, 'Epoch': 9}\n",
      "01/14/2023 18:08:09 - INFO - __main__ - Step 85 : {'lr': 1.3333333333333334e-06, 'samples': 170, 'steps': 5, 'loss/train': 7.668850898742676, 'Epoch': 9}\n",
      "01/14/2023 18:08:10 - INFO - __main__ - Step 86 : {'lr': 1.3333333333333334e-06, 'samples': 172, 'steps': 5, 'loss/train': 7.2306671142578125, 'Epoch': 9}\n",
      "01/14/2023 18:08:10 - INFO - __main__ - Step 87 : {'lr': 1.3333333333333334e-06, 'samples': 174, 'steps': 5, 'loss/train': 7.11314058303833, 'Epoch': 9}\n",
      "01/14/2023 18:08:10 - INFO - __main__ - Step 88 : {'lr': 1.3333333333333334e-06, 'samples': 176, 'steps': 5, 'loss/train': 6.840752601623535, 'Epoch': 9}\n",
      "01/14/2023 18:08:11 - INFO - __main__ - Step 89 : {'lr': 1.3333333333333334e-06, 'samples': 178, 'steps': 5, 'loss/train': 7.485252380371094, 'Epoch': 9}\n",
      "01/14/2023 18:08:11 - INFO - __main__ - Step 90 : {'lr': 1.3333333333333334e-06, 'samples': 180, 'steps': 5, 'loss/train': 7.561761856079102, 'Epoch': 9}\n",
      "01/14/2023 18:08:11 - INFO - __main__ - Step 91 : {'lr': 1.3333333333333334e-06, 'samples': 182, 'steps': 5, 'loss/train': 7.396234512329102, 'Epoch': 9}\n",
      "01/14/2023 18:08:12 - INFO - __main__ - Step 92 : {'lr': 1.3333333333333334e-06, 'samples': 184, 'steps': 5, 'loss/train': 7.757250785827637, 'Epoch': 9}\n",
      "01/14/2023 18:08:12 - INFO - __main__ - Step 93 : {'lr': 1.3333333333333334e-06, 'samples': 186, 'steps': 5, 'loss/train': 7.444291591644287, 'Epoch': 9}\n",
      "01/14/2023 18:08:13 - INFO - __main__ - Step 94 : {'lr': 1.3333333333333334e-06, 'samples': 188, 'steps': 5, 'loss/train': 7.833526134490967, 'Epoch': 9}\n",
      "01/14/2023 18:08:13 - INFO - __main__ - Step 95 : {'lr': 1.3333333333333334e-06, 'samples': 190, 'steps': 5, 'loss/train': 6.265997409820557, 'Epoch': 9}\n",
      "01/14/2023 18:08:13 - INFO - __main__ - Step 96 : {'lr': 1.3333333333333334e-06, 'samples': 192, 'steps': 5, 'loss/train': 6.526762962341309, 'Epoch': 9}\n",
      "01/14/2023 18:08:14 - INFO - __main__ - Step 97 : {'lr': 1.6000000000000001e-06, 'samples': 194, 'steps': 6, 'loss/train': 6.540919303894043, 'Epoch': 9}\n",
      "01/14/2023 18:08:14 - INFO - __main__ - Step 98 : {'lr': 1.6000000000000001e-06, 'samples': 196, 'steps': 6, 'loss/train': 6.750589370727539, 'Epoch': 9}\n",
      "01/14/2023 18:08:14 - INFO - __main__ - Step 99 : {'lr': 1.6000000000000001e-06, 'samples': 198, 'steps': 6, 'loss/train': 6.791421890258789, 'Epoch': 9}\n",
      "01/14/2023 18:08:15 - INFO - __main__ - Step 100 : {'lr': 1.6000000000000001e-06, 'samples': 200, 'steps': 6, 'loss/train': 6.922438144683838, 'Epoch': 9}\n",
      "01/14/2023 18:08:15 - INFO - __main__ - Step 101 : {'lr': 1.6000000000000001e-06, 'samples': 202, 'steps': 6, 'loss/train': 6.748508930206299, 'Epoch': 9}\n",
      "01/14/2023 18:08:15 - INFO - __main__ - Step 102 : {'lr': 1.6000000000000001e-06, 'samples': 204, 'steps': 6, 'loss/train': 6.92992639541626, 'Epoch': 9}\n",
      "01/14/2023 18:08:16 - INFO - __main__ - Step 103 : {'lr': 1.6000000000000001e-06, 'samples': 206, 'steps': 6, 'loss/train': 7.849296569824219, 'Epoch': 9}\n",
      "01/14/2023 18:08:16 - INFO - __main__ - Step 104 : {'lr': 1.6000000000000001e-06, 'samples': 208, 'steps': 6, 'loss/train': 7.380389213562012, 'Epoch': 9}\n",
      "01/14/2023 18:08:16 - INFO - __main__ - Step 105 : {'lr': 1.6000000000000001e-06, 'samples': 210, 'steps': 6, 'loss/train': 7.49713659286499, 'Epoch': 9}\n",
      "01/14/2023 18:08:17 - INFO - __main__ - Step 106 : {'lr': 1.6000000000000001e-06, 'samples': 212, 'steps': 6, 'loss/train': 8.124519348144531, 'Epoch': 9}\n",
      "01/14/2023 18:08:17 - INFO - __main__ - Step 107 : {'lr': 1.6000000000000001e-06, 'samples': 214, 'steps': 6, 'loss/train': 7.672889232635498, 'Epoch': 9}\n",
      "01/14/2023 18:08:18 - INFO - __main__ - Step 108 : {'lr': 1.6000000000000001e-06, 'samples': 216, 'steps': 6, 'loss/train': 7.688324928283691, 'Epoch': 9}\n",
      "01/14/2023 18:08:18 - INFO - __main__ - Step 109 : {'lr': 1.6000000000000001e-06, 'samples': 218, 'steps': 6, 'loss/train': 7.8935465812683105, 'Epoch': 9}\n",
      "01/14/2023 18:08:18 - INFO - __main__ - Step 110 : {'lr': 1.6000000000000001e-06, 'samples': 220, 'steps': 6, 'loss/train': 7.307553768157959, 'Epoch': 9}\n",
      "01/14/2023 18:08:19 - INFO - __main__ - Step 111 : {'lr': 1.6000000000000001e-06, 'samples': 222, 'steps': 6, 'loss/train': 7.93701171875, 'Epoch': 9}\n",
      "01/14/2023 18:08:19 - INFO - __main__ - Step 112 : {'lr': 1.6000000000000001e-06, 'samples': 224, 'steps': 6, 'loss/train': 8.17769718170166, 'Epoch': 9}\n",
      "01/14/2023 18:08:19 - INFO - __main__ - Step 113 : {'lr': 1.8666666666666669e-06, 'samples': 226, 'steps': 7, 'loss/train': 7.723967552185059, 'Epoch': 9}\n",
      "01/14/2023 18:08:20 - INFO - __main__ - Step 114 : {'lr': 1.8666666666666669e-06, 'samples': 228, 'steps': 7, 'loss/train': 7.393136978149414, 'Epoch': 9}\n",
      "01/14/2023 18:08:20 - INFO - __main__ - Step 115 : {'lr': 1.8666666666666669e-06, 'samples': 230, 'steps': 7, 'loss/train': 7.970983028411865, 'Epoch': 9}\n",
      "01/14/2023 18:08:20 - INFO - __main__ - Step 116 : {'lr': 1.8666666666666669e-06, 'samples': 232, 'steps': 7, 'loss/train': 7.274749279022217, 'Epoch': 9}\n",
      "01/14/2023 18:08:21 - INFO - __main__ - Step 117 : {'lr': 1.8666666666666669e-06, 'samples': 234, 'steps': 7, 'loss/train': 7.500196933746338, 'Epoch': 9}\n",
      "01/14/2023 18:08:21 - INFO - __main__ - Step 118 : {'lr': 1.8666666666666669e-06, 'samples': 236, 'steps': 7, 'loss/train': 7.024154186248779, 'Epoch': 9}\n",
      "01/14/2023 18:08:22 - INFO - __main__ - Step 119 : {'lr': 1.8666666666666669e-06, 'samples': 238, 'steps': 7, 'loss/train': 7.4580230712890625, 'Epoch': 9}\n",
      "01/14/2023 18:08:22 - INFO - __main__ - Step 120 : {'lr': 1.8666666666666669e-06, 'samples': 240, 'steps': 7, 'loss/train': 7.4217634201049805, 'Epoch': 9}\n",
      "01/14/2023 18:08:22 - INFO - __main__ - Step 121 : {'lr': 1.8666666666666669e-06, 'samples': 242, 'steps': 7, 'loss/train': 7.7379536628723145, 'Epoch': 9}\n",
      "01/14/2023 18:08:23 - INFO - __main__ - Step 122 : {'lr': 1.8666666666666669e-06, 'samples': 244, 'steps': 7, 'loss/train': 7.342875957489014, 'Epoch': 9}\n",
      "01/14/2023 18:08:23 - INFO - __main__ - Step 123 : {'lr': 1.8666666666666669e-06, 'samples': 246, 'steps': 7, 'loss/train': 7.953183174133301, 'Epoch': 9}\n",
      "01/14/2023 18:08:23 - INFO - __main__ - Step 124 : {'lr': 1.8666666666666669e-06, 'samples': 248, 'steps': 7, 'loss/train': 7.846719264984131, 'Epoch': 9}\n",
      "01/14/2023 18:08:24 - INFO - __main__ - Step 125 : {'lr': 1.8666666666666669e-06, 'samples': 250, 'steps': 7, 'loss/train': 7.235571384429932, 'Epoch': 9}\n",
      "01/14/2023 18:08:24 - INFO - __main__ - Step 126 : {'lr': 1.8666666666666669e-06, 'samples': 252, 'steps': 7, 'loss/train': 7.072314262390137, 'Epoch': 9}\n",
      "01/14/2023 18:08:25 - INFO - __main__ - Step 127 : {'lr': 1.8666666666666669e-06, 'samples': 254, 'steps': 7, 'loss/train': 7.1157426834106445, 'Epoch': 9}\n",
      "01/14/2023 18:08:25 - INFO - __main__ - Step 128 : {'lr': 1.8666666666666669e-06, 'samples': 256, 'steps': 7, 'loss/train': 7.0505523681640625, 'Epoch': 9}\n",
      "01/14/2023 18:08:25 - INFO - __main__ - Step 129 : {'lr': 2.1333333333333334e-06, 'samples': 258, 'steps': 8, 'loss/train': 6.755581855773926, 'Epoch': 9}\n",
      "01/14/2023 18:08:26 - INFO - __main__ - Step 130 : {'lr': 2.1333333333333334e-06, 'samples': 260, 'steps': 8, 'loss/train': 7.27499532699585, 'Epoch': 9}\n",
      "01/14/2023 18:08:26 - INFO - __main__ - Step 131 : {'lr': 2.1333333333333334e-06, 'samples': 262, 'steps': 8, 'loss/train': 7.673877716064453, 'Epoch': 9}\n",
      "01/14/2023 18:08:26 - INFO - __main__ - Step 132 : {'lr': 2.1333333333333334e-06, 'samples': 264, 'steps': 8, 'loss/train': 6.829722881317139, 'Epoch': 9}\n",
      "01/14/2023 18:08:27 - INFO - __main__ - Step 133 : {'lr': 2.1333333333333334e-06, 'samples': 266, 'steps': 8, 'loss/train': 7.670233726501465, 'Epoch': 9}\n",
      "01/14/2023 18:08:27 - INFO - __main__ - Step 134 : {'lr': 2.1333333333333334e-06, 'samples': 268, 'steps': 8, 'loss/train': 7.528183460235596, 'Epoch': 9}\n",
      "01/14/2023 18:08:28 - INFO - __main__ - Step 135 : {'lr': 2.1333333333333334e-06, 'samples': 270, 'steps': 8, 'loss/train': 8.052918434143066, 'Epoch': 9}\n",
      "01/14/2023 18:08:28 - INFO - __main__ - Step 136 : {'lr': 2.1333333333333334e-06, 'samples': 272, 'steps': 8, 'loss/train': 7.412044048309326, 'Epoch': 9}\n",
      "01/14/2023 18:08:28 - INFO - __main__ - Step 137 : {'lr': 2.1333333333333334e-06, 'samples': 274, 'steps': 8, 'loss/train': 7.500349998474121, 'Epoch': 9}\n",
      "01/14/2023 18:08:29 - INFO - __main__ - Step 138 : {'lr': 2.1333333333333334e-06, 'samples': 276, 'steps': 8, 'loss/train': 7.704355716705322, 'Epoch': 9}\n",
      "01/14/2023 18:08:29 - INFO - __main__ - Step 139 : {'lr': 2.1333333333333334e-06, 'samples': 278, 'steps': 8, 'loss/train': 7.524646759033203, 'Epoch': 9}\n",
      "01/14/2023 18:08:29 - INFO - __main__ - Step 140 : {'lr': 2.1333333333333334e-06, 'samples': 280, 'steps': 8, 'loss/train': 7.373774528503418, 'Epoch': 9}\n",
      "01/14/2023 18:08:30 - INFO - __main__ - Step 141 : {'lr': 2.1333333333333334e-06, 'samples': 282, 'steps': 8, 'loss/train': 7.652281761169434, 'Epoch': 9}\n",
      "01/14/2023 18:08:30 - INFO - __main__ - Step 142 : {'lr': 2.1333333333333334e-06, 'samples': 284, 'steps': 8, 'loss/train': 7.749631404876709, 'Epoch': 9}\n",
      "01/14/2023 18:08:31 - INFO - __main__ - Step 143 : {'lr': 2.1333333333333334e-06, 'samples': 286, 'steps': 8, 'loss/train': 7.7978515625, 'Epoch': 9}\n",
      "01/14/2023 18:08:31 - INFO - __main__ - Step 144 : {'lr': 2.1333333333333334e-06, 'samples': 288, 'steps': 8, 'loss/train': 7.724057197570801, 'Epoch': 9}\n",
      "01/14/2023 18:08:31 - INFO - __main__ - Step 145 : {'lr': 2.4000000000000003e-06, 'samples': 290, 'steps': 9, 'loss/train': 7.540308952331543, 'Epoch': 9}\n",
      "01/14/2023 18:08:32 - INFO - __main__ - Step 146 : {'lr': 2.4000000000000003e-06, 'samples': 292, 'steps': 9, 'loss/train': 6.542361259460449, 'Epoch': 9}\n",
      "01/14/2023 18:08:32 - INFO - __main__ - Step 147 : {'lr': 2.4000000000000003e-06, 'samples': 294, 'steps': 9, 'loss/train': 7.691080570220947, 'Epoch': 9}\n",
      "01/14/2023 18:08:32 - INFO - __main__ - Step 148 : {'lr': 2.4000000000000003e-06, 'samples': 296, 'steps': 9, 'loss/train': 7.741391181945801, 'Epoch': 9}\n",
      "01/14/2023 18:08:33 - INFO - __main__ - Step 149 : {'lr': 2.4000000000000003e-06, 'samples': 298, 'steps': 9, 'loss/train': 7.4856390953063965, 'Epoch': 9}\n",
      "01/14/2023 18:08:33 - INFO - __main__ - Step 150 : {'lr': 2.4000000000000003e-06, 'samples': 300, 'steps': 9, 'loss/train': 7.458911418914795, 'Epoch': 9}\n",
      "01/14/2023 18:08:34 - INFO - __main__ - Step 151 : {'lr': 2.4000000000000003e-06, 'samples': 302, 'steps': 9, 'loss/train': 7.416997909545898, 'Epoch': 9}\n",
      "01/14/2023 18:08:34 - INFO - __main__ - Step 152 : {'lr': 2.4000000000000003e-06, 'samples': 304, 'steps': 9, 'loss/train': 7.703576564788818, 'Epoch': 9}\n",
      "01/14/2023 18:08:34 - INFO - __main__ - Step 153 : {'lr': 2.4000000000000003e-06, 'samples': 306, 'steps': 9, 'loss/train': 7.95172119140625, 'Epoch': 9}\n",
      "01/14/2023 18:08:35 - INFO - __main__ - Step 154 : {'lr': 2.4000000000000003e-06, 'samples': 308, 'steps': 9, 'loss/train': 7.70371150970459, 'Epoch': 9}\n",
      "01/14/2023 18:08:35 - INFO - __main__ - Step 155 : {'lr': 2.4000000000000003e-06, 'samples': 310, 'steps': 9, 'loss/train': 7.697678089141846, 'Epoch': 9}\n",
      "01/14/2023 18:08:35 - INFO - __main__ - Step 156 : {'lr': 2.4000000000000003e-06, 'samples': 312, 'steps': 9, 'loss/train': 7.112910270690918, 'Epoch': 9}\n",
      "01/14/2023 18:08:36 - INFO - __main__ - Step 157 : {'lr': 2.4000000000000003e-06, 'samples': 314, 'steps': 9, 'loss/train': 7.671868801116943, 'Epoch': 9}\n",
      "01/14/2023 18:08:36 - INFO - __main__ - Step 158 : {'lr': 2.4000000000000003e-06, 'samples': 316, 'steps': 9, 'loss/train': 7.1829376220703125, 'Epoch': 9}\n",
      "01/14/2023 18:08:36 - INFO - __main__ - Step 159 : {'lr': 2.4000000000000003e-06, 'samples': 318, 'steps': 9, 'loss/train': 7.1326375007629395, 'Epoch': 9}\n",
      "01/14/2023 18:08:37 - INFO - __main__ - Step 160 : {'lr': 2.4000000000000003e-06, 'samples': 320, 'steps': 9, 'loss/train': 7.403798580169678, 'Epoch': 9}\n",
      "01/14/2023 18:08:37 - INFO - __main__ - Step 161 : {'lr': 2.666666666666667e-06, 'samples': 322, 'steps': 10, 'loss/train': 7.832395553588867, 'Epoch': 9}\n",
      "01/14/2023 18:08:38 - INFO - __main__ - Step 162 : {'lr': 2.666666666666667e-06, 'samples': 324, 'steps': 10, 'loss/train': 7.79299259185791, 'Epoch': 9}\n",
      "01/14/2023 18:08:38 - INFO - __main__ - Step 163 : {'lr': 2.666666666666667e-06, 'samples': 326, 'steps': 10, 'loss/train': 6.937327861785889, 'Epoch': 9}\n",
      "01/14/2023 18:08:38 - INFO - __main__ - Step 164 : {'lr': 2.666666666666667e-06, 'samples': 328, 'steps': 10, 'loss/train': 7.225043296813965, 'Epoch': 9}\n",
      "01/14/2023 18:08:39 - INFO - __main__ - Step 165 : {'lr': 2.666666666666667e-06, 'samples': 330, 'steps': 10, 'loss/train': 7.277877330780029, 'Epoch': 9}\n",
      "01/14/2023 18:08:39 - INFO - __main__ - Step 166 : {'lr': 2.666666666666667e-06, 'samples': 332, 'steps': 10, 'loss/train': 7.44076681137085, 'Epoch': 9}\n",
      "01/14/2023 18:08:39 - INFO - __main__ - Step 167 : {'lr': 2.666666666666667e-06, 'samples': 334, 'steps': 10, 'loss/train': 7.345345497131348, 'Epoch': 9}\n",
      "01/14/2023 18:08:40 - INFO - __main__ - Step 168 : {'lr': 2.666666666666667e-06, 'samples': 336, 'steps': 10, 'loss/train': 7.134876728057861, 'Epoch': 9}\n",
      "01/14/2023 18:08:40 - INFO - __main__ - Step 169 : {'lr': 2.666666666666667e-06, 'samples': 338, 'steps': 10, 'loss/train': 7.147823333740234, 'Epoch': 9}\n",
      "01/14/2023 18:08:40 - INFO - __main__ - Step 170 : {'lr': 2.666666666666667e-06, 'samples': 340, 'steps': 10, 'loss/train': 7.121021270751953, 'Epoch': 9}\n",
      "01/14/2023 18:08:41 - INFO - __main__ - Step 171 : {'lr': 2.666666666666667e-06, 'samples': 342, 'steps': 10, 'loss/train': 7.305181980133057, 'Epoch': 9}\n",
      "01/14/2023 18:08:41 - INFO - __main__ - Step 172 : {'lr': 2.666666666666667e-06, 'samples': 344, 'steps': 10, 'loss/train': 7.819102764129639, 'Epoch': 9}\n",
      "01/14/2023 18:08:42 - INFO - __main__ - Step 173 : {'lr': 2.666666666666667e-06, 'samples': 346, 'steps': 10, 'loss/train': 8.683500289916992, 'Epoch': 9}\n",
      "01/14/2023 18:08:42 - INFO - __main__ - Step 174 : {'lr': 2.666666666666667e-06, 'samples': 348, 'steps': 10, 'loss/train': 7.9079742431640625, 'Epoch': 9}\n",
      "01/14/2023 18:08:42 - INFO - __main__ - Step 175 : {'lr': 2.666666666666667e-06, 'samples': 350, 'steps': 10, 'loss/train': 7.305041313171387, 'Epoch': 9}\n",
      "01/14/2023 18:08:43 - INFO - __main__ - Step 176 : {'lr': 2.666666666666667e-06, 'samples': 352, 'steps': 10, 'loss/train': 7.5235161781311035, 'Epoch': 9}\n",
      "01/14/2023 18:08:43 - INFO - __main__ - Step 177 : {'lr': 2.9333333333333333e-06, 'samples': 354, 'steps': 11, 'loss/train': 7.882259845733643, 'Epoch': 9}\n",
      "01/14/2023 18:08:43 - INFO - __main__ - Step 178 : {'lr': 2.9333333333333333e-06, 'samples': 356, 'steps': 11, 'loss/train': 7.61264705657959, 'Epoch': 9}\n",
      "01/14/2023 18:08:44 - INFO - __main__ - Step 179 : {'lr': 2.9333333333333333e-06, 'samples': 358, 'steps': 11, 'loss/train': 7.802361965179443, 'Epoch': 9}\n",
      "01/14/2023 18:08:44 - INFO - __main__ - Step 180 : {'lr': 2.9333333333333333e-06, 'samples': 360, 'steps': 11, 'loss/train': 8.0438232421875, 'Epoch': 9}\n",
      "01/14/2023 18:08:45 - INFO - __main__ - Step 181 : {'lr': 2.9333333333333333e-06, 'samples': 362, 'steps': 11, 'loss/train': 7.621889114379883, 'Epoch': 9}\n",
      "01/14/2023 18:08:45 - INFO - __main__ - Step 182 : {'lr': 2.9333333333333333e-06, 'samples': 364, 'steps': 11, 'loss/train': 7.618149757385254, 'Epoch': 9}\n",
      "01/14/2023 18:08:45 - INFO - __main__ - Step 183 : {'lr': 2.9333333333333333e-06, 'samples': 366, 'steps': 11, 'loss/train': 7.756720066070557, 'Epoch': 9}\n",
      "01/14/2023 18:08:46 - INFO - __main__ - Step 184 : {'lr': 2.9333333333333333e-06, 'samples': 368, 'steps': 11, 'loss/train': 6.624007225036621, 'Epoch': 9}\n",
      "01/14/2023 18:08:46 - INFO - __main__ - Step 185 : {'lr': 2.9333333333333333e-06, 'samples': 370, 'steps': 11, 'loss/train': 7.179981231689453, 'Epoch': 9}\n",
      "01/14/2023 18:08:46 - INFO - __main__ - Step 186 : {'lr': 2.9333333333333333e-06, 'samples': 372, 'steps': 11, 'loss/train': 7.4599761962890625, 'Epoch': 9}\n",
      "01/14/2023 18:08:47 - INFO - __main__ - Step 187 : {'lr': 2.9333333333333333e-06, 'samples': 374, 'steps': 11, 'loss/train': 7.928591728210449, 'Epoch': 9}\n",
      "01/14/2023 18:08:47 - INFO - __main__ - Step 188 : {'lr': 2.9333333333333333e-06, 'samples': 376, 'steps': 11, 'loss/train': 7.88350248336792, 'Epoch': 9}\n",
      "01/14/2023 18:08:47 - INFO - __main__ - Step 189 : {'lr': 2.9333333333333333e-06, 'samples': 378, 'steps': 11, 'loss/train': 7.45850944519043, 'Epoch': 9}\n",
      "01/14/2023 18:08:48 - INFO - __main__ - Step 190 : {'lr': 2.9333333333333333e-06, 'samples': 380, 'steps': 11, 'loss/train': 8.1674222946167, 'Epoch': 9}\n",
      "01/14/2023 18:08:48 - INFO - __main__ - Step 191 : {'lr': 2.9333333333333333e-06, 'samples': 382, 'steps': 11, 'loss/train': 8.177845001220703, 'Epoch': 9}\n",
      "01/14/2023 18:08:49 - INFO - __main__ - Step 192 : {'lr': 2.9333333333333333e-06, 'samples': 384, 'steps': 11, 'loss/train': 6.570454120635986, 'Epoch': 9}\n",
      "01/14/2023 18:08:49 - INFO - __main__ - Step 193 : {'lr': 3.2000000000000003e-06, 'samples': 386, 'steps': 12, 'loss/train': 7.157106399536133, 'Epoch': 9}\n",
      "01/14/2023 18:08:49 - INFO - __main__ - Step 194 : {'lr': 3.2000000000000003e-06, 'samples': 388, 'steps': 12, 'loss/train': 6.953357696533203, 'Epoch': 9}\n",
      "01/14/2023 18:08:50 - INFO - __main__ - Step 195 : {'lr': 3.2000000000000003e-06, 'samples': 390, 'steps': 12, 'loss/train': 7.342602252960205, 'Epoch': 9}\n",
      "01/14/2023 18:08:50 - INFO - __main__ - Step 196 : {'lr': 3.2000000000000003e-06, 'samples': 392, 'steps': 12, 'loss/train': 7.364988803863525, 'Epoch': 9}\n",
      "01/14/2023 18:08:50 - INFO - __main__ - Step 197 : {'lr': 3.2000000000000003e-06, 'samples': 394, 'steps': 12, 'loss/train': 6.774821758270264, 'Epoch': 9}\n",
      "01/14/2023 18:08:51 - INFO - __main__ - Step 198 : {'lr': 3.2000000000000003e-06, 'samples': 396, 'steps': 12, 'loss/train': 7.981342792510986, 'Epoch': 9}\n",
      "01/14/2023 18:08:51 - INFO - __main__ - Step 199 : {'lr': 3.2000000000000003e-06, 'samples': 398, 'steps': 12, 'loss/train': 7.162082672119141, 'Epoch': 9}\n",
      "01/14/2023 18:08:52 - INFO - __main__ - Step 200 : {'lr': 3.2000000000000003e-06, 'samples': 400, 'steps': 12, 'loss/train': 7.580358505249023, 'Epoch': 9}\n",
      "01/14/2023 18:08:52 - INFO - __main__ - Step 201 : {'lr': 3.2000000000000003e-06, 'samples': 402, 'steps': 12, 'loss/train': 8.026931762695312, 'Epoch': 9}\n",
      "01/14/2023 18:08:52 - INFO - __main__ - Step 202 : {'lr': 3.2000000000000003e-06, 'samples': 404, 'steps': 12, 'loss/train': 7.648087978363037, 'Epoch': 9}\n",
      "01/14/2023 18:08:53 - INFO - __main__ - Step 203 : {'lr': 3.2000000000000003e-06, 'samples': 406, 'steps': 12, 'loss/train': 7.275184631347656, 'Epoch': 9}\n",
      "01/14/2023 18:08:53 - INFO - __main__ - Step 204 : {'lr': 3.2000000000000003e-06, 'samples': 408, 'steps': 12, 'loss/train': 7.361618518829346, 'Epoch': 9}\n",
      "01/14/2023 18:08:53 - INFO - __main__ - Step 205 : {'lr': 3.2000000000000003e-06, 'samples': 410, 'steps': 12, 'loss/train': 6.961239337921143, 'Epoch': 9}\n",
      "01/14/2023 18:08:54 - INFO - __main__ - Step 206 : {'lr': 3.2000000000000003e-06, 'samples': 412, 'steps': 12, 'loss/train': 7.208254814147949, 'Epoch': 9}\n",
      "01/14/2023 18:08:54 - INFO - __main__ - Step 207 : {'lr': 3.2000000000000003e-06, 'samples': 414, 'steps': 12, 'loss/train': 7.175614833831787, 'Epoch': 9}\n",
      "01/14/2023 18:08:54 - INFO - __main__ - Step 208 : {'lr': 3.2000000000000003e-06, 'samples': 416, 'steps': 12, 'loss/train': 6.818521976470947, 'Epoch': 9}\n",
      "01/14/2023 18:08:55 - INFO - __main__ - Step 209 : {'lr': 3.466666666666667e-06, 'samples': 418, 'steps': 13, 'loss/train': 7.365897178649902, 'Epoch': 9}\n",
      "01/14/2023 18:08:55 - INFO - __main__ - Step 210 : {'lr': 3.466666666666667e-06, 'samples': 420, 'steps': 13, 'loss/train': 7.320460796356201, 'Epoch': 9}\n",
      "01/14/2023 18:08:56 - INFO - __main__ - Step 211 : {'lr': 3.466666666666667e-06, 'samples': 422, 'steps': 13, 'loss/train': 7.380666255950928, 'Epoch': 9}\n",
      "01/14/2023 18:08:56 - INFO - __main__ - Step 212 : {'lr': 3.466666666666667e-06, 'samples': 424, 'steps': 13, 'loss/train': 7.58016300201416, 'Epoch': 9}\n",
      "01/14/2023 18:08:56 - INFO - __main__ - Step 213 : {'lr': 3.466666666666667e-06, 'samples': 426, 'steps': 13, 'loss/train': 7.361867904663086, 'Epoch': 9}\n",
      "01/14/2023 18:08:57 - INFO - __main__ - Step 214 : {'lr': 3.466666666666667e-06, 'samples': 428, 'steps': 13, 'loss/train': 7.815199375152588, 'Epoch': 9}\n",
      "01/14/2023 18:08:57 - INFO - __main__ - Step 215 : {'lr': 3.466666666666667e-06, 'samples': 430, 'steps': 13, 'loss/train': 7.394526481628418, 'Epoch': 9}\n",
      "01/14/2023 18:08:58 - INFO - __main__ - Step 216 : {'lr': 3.466666666666667e-06, 'samples': 432, 'steps': 13, 'loss/train': 7.460639476776123, 'Epoch': 9}\n",
      "01/14/2023 18:08:58 - INFO - __main__ - Step 217 : {'lr': 3.466666666666667e-06, 'samples': 434, 'steps': 13, 'loss/train': 7.390893459320068, 'Epoch': 9}\n",
      "01/14/2023 18:08:58 - INFO - __main__ - Step 218 : {'lr': 3.466666666666667e-06, 'samples': 436, 'steps': 13, 'loss/train': 7.317964553833008, 'Epoch': 9}\n",
      "01/14/2023 18:08:59 - INFO - __main__ - Step 219 : {'lr': 3.466666666666667e-06, 'samples': 438, 'steps': 13, 'loss/train': 7.511956214904785, 'Epoch': 9}\n",
      "01/14/2023 18:08:59 - INFO - __main__ - Step 220 : {'lr': 3.466666666666667e-06, 'samples': 440, 'steps': 13, 'loss/train': 7.700245380401611, 'Epoch': 9}\n",
      "01/14/2023 18:09:00 - INFO - __main__ - Step 221 : {'lr': 3.466666666666667e-06, 'samples': 442, 'steps': 13, 'loss/train': 7.5969719886779785, 'Epoch': 9}\n",
      "01/14/2023 18:09:00 - INFO - __main__ - Step 222 : {'lr': 3.466666666666667e-06, 'samples': 444, 'steps': 13, 'loss/train': 7.295197010040283, 'Epoch': 9}\n",
      "01/14/2023 18:09:00 - INFO - __main__ - Step 223 : {'lr': 3.466666666666667e-06, 'samples': 446, 'steps': 13, 'loss/train': 7.424391269683838, 'Epoch': 9}\n",
      "01/14/2023 18:09:01 - INFO - __main__ - Step 224 : {'lr': 3.466666666666667e-06, 'samples': 448, 'steps': 13, 'loss/train': 7.295068264007568, 'Epoch': 9}\n",
      "01/14/2023 18:09:01 - INFO - __main__ - Step 225 : {'lr': 3.7333333333333337e-06, 'samples': 450, 'steps': 14, 'loss/train': 7.251547813415527, 'Epoch': 9}\n",
      "01/14/2023 18:09:02 - INFO - __main__ - Step 226 : {'lr': 3.7333333333333337e-06, 'samples': 452, 'steps': 14, 'loss/train': 7.416888236999512, 'Epoch': 9}\n",
      "01/14/2023 18:09:02 - INFO - __main__ - Step 227 : {'lr': 3.7333333333333337e-06, 'samples': 454, 'steps': 14, 'loss/train': 7.377369403839111, 'Epoch': 9}\n",
      "01/14/2023 18:09:02 - INFO - __main__ - Step 228 : {'lr': 3.7333333333333337e-06, 'samples': 456, 'steps': 14, 'loss/train': 7.378418922424316, 'Epoch': 9}\n",
      "01/14/2023 18:09:03 - INFO - __main__ - Step 229 : {'lr': 3.7333333333333337e-06, 'samples': 458, 'steps': 14, 'loss/train': 7.286959171295166, 'Epoch': 9}\n",
      "01/14/2023 18:09:03 - INFO - __main__ - Step 230 : {'lr': 3.7333333333333337e-06, 'samples': 460, 'steps': 14, 'loss/train': 7.286005973815918, 'Epoch': 9}\n",
      "01/14/2023 18:09:03 - INFO - __main__ - Step 231 : {'lr': 3.7333333333333337e-06, 'samples': 462, 'steps': 14, 'loss/train': 7.552515029907227, 'Epoch': 9}\n",
      "01/14/2023 18:09:04 - INFO - __main__ - Step 232 : {'lr': 3.7333333333333337e-06, 'samples': 464, 'steps': 14, 'loss/train': 6.818800449371338, 'Epoch': 9}\n",
      "01/14/2023 18:09:04 - INFO - __main__ - Step 233 : {'lr': 3.7333333333333337e-06, 'samples': 466, 'steps': 14, 'loss/train': 6.996762752532959, 'Epoch': 9}\n",
      "01/14/2023 18:09:05 - INFO - __main__ - Step 234 : {'lr': 3.7333333333333337e-06, 'samples': 468, 'steps': 14, 'loss/train': 6.513391971588135, 'Epoch': 9}\n",
      "01/14/2023 18:09:05 - INFO - __main__ - Step 235 : {'lr': 3.7333333333333337e-06, 'samples': 470, 'steps': 14, 'loss/train': 6.615223407745361, 'Epoch': 9}\n",
      "01/14/2023 18:09:05 - INFO - __main__ - Step 236 : {'lr': 3.7333333333333337e-06, 'samples': 472, 'steps': 14, 'loss/train': 6.527062892913818, 'Epoch': 9}\n",
      "01/14/2023 18:09:06 - INFO - __main__ - Step 237 : {'lr': 3.7333333333333337e-06, 'samples': 474, 'steps': 14, 'loss/train': 6.3572611808776855, 'Epoch': 9}\n",
      "01/14/2023 18:09:06 - INFO - __main__ - Step 238 : {'lr': 3.7333333333333337e-06, 'samples': 476, 'steps': 14, 'loss/train': 6.408398151397705, 'Epoch': 9}\n",
      "01/14/2023 18:09:07 - INFO - __main__ - Step 239 : {'lr': 3.7333333333333337e-06, 'samples': 478, 'steps': 14, 'loss/train': 6.1125593185424805, 'Epoch': 9}\n",
      "01/14/2023 18:09:07 - INFO - __main__ - Step 240 : {'lr': 3.7333333333333337e-06, 'samples': 480, 'steps': 14, 'loss/train': 7.496349334716797, 'Epoch': 9}\n",
      "01/14/2023 18:09:07 - INFO - __main__ - Step 241 : {'lr': 4.000000000000001e-06, 'samples': 482, 'steps': 15, 'loss/train': 7.7676849365234375, 'Epoch': 9}\n",
      "01/14/2023 18:09:08 - INFO - __main__ - Step 242 : {'lr': 4.000000000000001e-06, 'samples': 484, 'steps': 15, 'loss/train': 8.611621856689453, 'Epoch': 9}\n",
      "01/14/2023 18:09:08 - INFO - __main__ - Step 243 : {'lr': 4.000000000000001e-06, 'samples': 486, 'steps': 15, 'loss/train': 8.61599349975586, 'Epoch': 9}\n",
      "01/14/2023 18:09:09 - INFO - __main__ - Step 244 : {'lr': 4.000000000000001e-06, 'samples': 488, 'steps': 15, 'loss/train': 8.254003524780273, 'Epoch': 9}\n",
      "01/14/2023 18:09:09 - INFO - __main__ - Step 245 : {'lr': 4.000000000000001e-06, 'samples': 490, 'steps': 15, 'loss/train': 8.232810020446777, 'Epoch': 9}\n",
      "01/14/2023 18:09:09 - INFO - __main__ - Step 246 : {'lr': 4.000000000000001e-06, 'samples': 492, 'steps': 15, 'loss/train': 7.856908798217773, 'Epoch': 9}\n",
      "01/14/2023 18:09:10 - INFO - __main__ - Step 247 : {'lr': 4.000000000000001e-06, 'samples': 494, 'steps': 15, 'loss/train': 7.855309963226318, 'Epoch': 9}\n",
      "01/14/2023 18:09:10 - INFO - __main__ - Step 248 : {'lr': 4.000000000000001e-06, 'samples': 496, 'steps': 15, 'loss/train': 7.833604335784912, 'Epoch': 9}\n",
      "01/14/2023 18:09:10 - INFO - __main__ - Step 249 : {'lr': 4.000000000000001e-06, 'samples': 498, 'steps': 15, 'loss/train': 6.935964107513428, 'Epoch': 9}\n",
      "01/14/2023 18:09:11 - INFO - __main__ - Step 250 : {'lr': 4.000000000000001e-06, 'samples': 500, 'steps': 15, 'loss/train': 6.176621913909912, 'Epoch': 9}\n",
      "01/14/2023 18:09:11 - INFO - __main__ - Step 251 : {'lr': 4.000000000000001e-06, 'samples': 502, 'steps': 15, 'loss/train': 7.2444586753845215, 'Epoch': 9}\n",
      "01/14/2023 18:09:12 - INFO - __main__ - Step 252 : {'lr': 4.000000000000001e-06, 'samples': 504, 'steps': 15, 'loss/train': 7.400568008422852, 'Epoch': 9}\n",
      "01/14/2023 18:09:12 - INFO - __main__ - Step 253 : {'lr': 4.000000000000001e-06, 'samples': 506, 'steps': 15, 'loss/train': 7.80948543548584, 'Epoch': 9}\n",
      "01/14/2023 18:09:12 - INFO - __main__ - Step 254 : {'lr': 4.000000000000001e-06, 'samples': 508, 'steps': 15, 'loss/train': 7.51212215423584, 'Epoch': 9}\n",
      "01/14/2023 18:09:13 - INFO - __main__ - Step 255 : {'lr': 4.000000000000001e-06, 'samples': 510, 'steps': 15, 'loss/train': 6.641234874725342, 'Epoch': 9}\n",
      "01/14/2023 18:09:13 - INFO - __main__ - Step 256 : {'lr': 4.000000000000001e-06, 'samples': 512, 'steps': 15, 'loss/train': 8.066107749938965, 'Epoch': 9}\n",
      "01/14/2023 18:09:13 - INFO - __main__ - Step 257 : {'lr': 4.266666666666667e-06, 'samples': 514, 'steps': 16, 'loss/train': 7.33133602142334, 'Epoch': 9}\n",
      "01/14/2023 18:09:14 - INFO - __main__ - Step 258 : {'lr': 4.266666666666667e-06, 'samples': 516, 'steps': 16, 'loss/train': 8.33646297454834, 'Epoch': 9}\n",
      "01/14/2023 18:09:14 - INFO - __main__ - Step 259 : {'lr': 4.266666666666667e-06, 'samples': 518, 'steps': 16, 'loss/train': 8.173118591308594, 'Epoch': 9}\n",
      "01/14/2023 18:09:15 - INFO - __main__ - Step 260 : {'lr': 4.266666666666667e-06, 'samples': 520, 'steps': 16, 'loss/train': 7.925318241119385, 'Epoch': 9}\n",
      "01/14/2023 18:09:15 - INFO - __main__ - Step 261 : {'lr': 4.266666666666667e-06, 'samples': 522, 'steps': 16, 'loss/train': 7.915499210357666, 'Epoch': 9}\n",
      "01/14/2023 18:09:15 - INFO - __main__ - Step 262 : {'lr': 4.266666666666667e-06, 'samples': 524, 'steps': 16, 'loss/train': 7.372642517089844, 'Epoch': 9}\n",
      "01/14/2023 18:09:16 - INFO - __main__ - Step 263 : {'lr': 4.266666666666667e-06, 'samples': 526, 'steps': 16, 'loss/train': 7.484273433685303, 'Epoch': 9}\n",
      "01/14/2023 18:09:16 - INFO - __main__ - Step 264 : {'lr': 4.266666666666667e-06, 'samples': 528, 'steps': 16, 'loss/train': 7.542422771453857, 'Epoch': 9}\n",
      "01/14/2023 18:09:16 - INFO - __main__ - Step 265 : {'lr': 4.266666666666667e-06, 'samples': 530, 'steps': 16, 'loss/train': 7.33081579208374, 'Epoch': 9}\n",
      "01/14/2023 18:09:17 - INFO - __main__ - Step 266 : {'lr': 4.266666666666667e-06, 'samples': 532, 'steps': 16, 'loss/train': 6.864017486572266, 'Epoch': 9}\n",
      "01/14/2023 18:09:17 - INFO - __main__ - Step 267 : {'lr': 4.266666666666667e-06, 'samples': 534, 'steps': 16, 'loss/train': 6.986577033996582, 'Epoch': 9}\n",
      "01/14/2023 18:09:18 - INFO - __main__ - Step 268 : {'lr': 4.266666666666667e-06, 'samples': 536, 'steps': 16, 'loss/train': 7.056772232055664, 'Epoch': 9}\n",
      "01/14/2023 18:09:18 - INFO - __main__ - Step 269 : {'lr': 4.266666666666667e-06, 'samples': 538, 'steps': 16, 'loss/train': 6.702352523803711, 'Epoch': 9}\n",
      "01/14/2023 18:09:18 - INFO - __main__ - Step 270 : {'lr': 4.266666666666667e-06, 'samples': 540, 'steps': 16, 'loss/train': 6.554988384246826, 'Epoch': 9}\n",
      "01/14/2023 18:09:19 - INFO - __main__ - Step 271 : {'lr': 4.266666666666667e-06, 'samples': 542, 'steps': 16, 'loss/train': 7.0795392990112305, 'Epoch': 9}\n",
      "01/14/2023 18:09:19 - INFO - __main__ - Step 272 : {'lr': 4.266666666666667e-06, 'samples': 544, 'steps': 16, 'loss/train': 7.844414234161377, 'Epoch': 9}\n",
      "01/14/2023 18:09:19 - INFO - __main__ - Step 273 : {'lr': 4.533333333333334e-06, 'samples': 546, 'steps': 17, 'loss/train': 7.674023151397705, 'Epoch': 9}\n",
      "01/14/2023 18:09:20 - INFO - __main__ - Step 274 : {'lr': 4.533333333333334e-06, 'samples': 548, 'steps': 17, 'loss/train': 7.5075297355651855, 'Epoch': 9}\n",
      "01/14/2023 18:09:20 - INFO - __main__ - Step 275 : {'lr': 4.533333333333334e-06, 'samples': 550, 'steps': 17, 'loss/train': 6.990286350250244, 'Epoch': 9}\n",
      "01/14/2023 18:09:21 - INFO - __main__ - Step 276 : {'lr': 4.533333333333334e-06, 'samples': 552, 'steps': 17, 'loss/train': 7.212204456329346, 'Epoch': 9}\n",
      "01/14/2023 18:09:21 - INFO - __main__ - Step 277 : {'lr': 4.533333333333334e-06, 'samples': 554, 'steps': 17, 'loss/train': 7.282886981964111, 'Epoch': 9}\n",
      "01/14/2023 18:09:21 - INFO - __main__ - Step 278 : {'lr': 4.533333333333334e-06, 'samples': 556, 'steps': 17, 'loss/train': 7.4299798011779785, 'Epoch': 9}\n",
      "01/14/2023 18:09:22 - INFO - __main__ - Step 279 : {'lr': 4.533333333333334e-06, 'samples': 558, 'steps': 17, 'loss/train': 7.321939945220947, 'Epoch': 9}\n",
      "01/14/2023 18:09:22 - INFO - __main__ - Step 280 : {'lr': 4.533333333333334e-06, 'samples': 560, 'steps': 17, 'loss/train': 7.458266735076904, 'Epoch': 9}\n",
      "01/14/2023 18:09:22 - INFO - __main__ - Step 281 : {'lr': 4.533333333333334e-06, 'samples': 562, 'steps': 17, 'loss/train': 7.395112991333008, 'Epoch': 9}\n",
      "01/14/2023 18:09:23 - INFO - __main__ - Step 282 : {'lr': 4.533333333333334e-06, 'samples': 564, 'steps': 17, 'loss/train': 7.103954792022705, 'Epoch': 9}\n",
      "01/14/2023 18:09:23 - INFO - __main__ - Step 283 : {'lr': 4.533333333333334e-06, 'samples': 566, 'steps': 17, 'loss/train': 7.523536682128906, 'Epoch': 9}\n",
      "01/14/2023 18:09:24 - INFO - __main__ - Step 284 : {'lr': 4.533333333333334e-06, 'samples': 568, 'steps': 17, 'loss/train': 8.231925964355469, 'Epoch': 9}\n",
      "01/14/2023 18:09:24 - INFO - __main__ - Step 285 : {'lr': 4.533333333333334e-06, 'samples': 570, 'steps': 17, 'loss/train': 7.872365474700928, 'Epoch': 9}\n",
      "01/14/2023 18:09:24 - INFO - __main__ - Step 286 : {'lr': 4.533333333333334e-06, 'samples': 572, 'steps': 17, 'loss/train': 8.17374324798584, 'Epoch': 9}\n",
      "01/14/2023 18:09:25 - INFO - __main__ - Step 287 : {'lr': 4.533333333333334e-06, 'samples': 574, 'steps': 17, 'loss/train': 7.943079471588135, 'Epoch': 9}\n",
      "01/14/2023 18:09:25 - INFO - __main__ - Step 288 : {'lr': 4.533333333333334e-06, 'samples': 576, 'steps': 17, 'loss/train': 7.776722431182861, 'Epoch': 9}\n",
      "01/14/2023 18:09:25 - INFO - __main__ - Step 289 : {'lr': 4.800000000000001e-06, 'samples': 578, 'steps': 18, 'loss/train': 7.385337829589844, 'Epoch': 9}\n",
      "01/14/2023 18:09:26 - INFO - __main__ - Step 290 : {'lr': 4.800000000000001e-06, 'samples': 580, 'steps': 18, 'loss/train': 7.456142425537109, 'Epoch': 9}\n",
      "01/14/2023 18:09:26 - INFO - __main__ - Step 291 : {'lr': 4.800000000000001e-06, 'samples': 582, 'steps': 18, 'loss/train': 7.290007591247559, 'Epoch': 9}\n",
      "01/14/2023 18:09:27 - INFO - __main__ - Step 292 : {'lr': 4.800000000000001e-06, 'samples': 584, 'steps': 18, 'loss/train': 7.089240550994873, 'Epoch': 9}\n",
      "01/14/2023 18:09:27 - INFO - __main__ - Step 293 : {'lr': 4.800000000000001e-06, 'samples': 586, 'steps': 18, 'loss/train': 7.3886027336120605, 'Epoch': 9}\n",
      "01/14/2023 18:09:27 - INFO - __main__ - Step 294 : {'lr': 4.800000000000001e-06, 'samples': 588, 'steps': 18, 'loss/train': 7.571410179138184, 'Epoch': 9}\n",
      "01/14/2023 18:09:28 - INFO - __main__ - Step 295 : {'lr': 4.800000000000001e-06, 'samples': 590, 'steps': 18, 'loss/train': 7.297519207000732, 'Epoch': 9}\n",
      "01/14/2023 18:09:28 - INFO - __main__ - Step 296 : {'lr': 4.800000000000001e-06, 'samples': 592, 'steps': 18, 'loss/train': 7.469303131103516, 'Epoch': 9}\n",
      "01/14/2023 18:09:28 - INFO - __main__ - Step 297 : {'lr': 4.800000000000001e-06, 'samples': 594, 'steps': 18, 'loss/train': 7.801031112670898, 'Epoch': 9}\n",
      "01/14/2023 18:09:29 - INFO - __main__ - Step 298 : {'lr': 4.800000000000001e-06, 'samples': 596, 'steps': 18, 'loss/train': 7.4228973388671875, 'Epoch': 9}\n",
      "01/14/2023 18:09:29 - INFO - __main__ - Step 299 : {'lr': 4.800000000000001e-06, 'samples': 598, 'steps': 18, 'loss/train': 7.644437313079834, 'Epoch': 9}\n",
      "01/14/2023 18:09:29 - INFO - __main__ - Step 300 : {'lr': 4.800000000000001e-06, 'samples': 600, 'steps': 18, 'loss/train': 7.706788063049316, 'Epoch': 9}\n",
      "01/14/2023 18:09:30 - INFO - __main__ - Step 301 : {'lr': 4.800000000000001e-06, 'samples': 602, 'steps': 18, 'loss/train': 7.777374744415283, 'Epoch': 9}\n",
      "01/14/2023 18:09:30 - INFO - __main__ - Step 302 : {'lr': 4.800000000000001e-06, 'samples': 604, 'steps': 18, 'loss/train': 7.839656352996826, 'Epoch': 9}\n",
      "01/14/2023 18:09:31 - INFO - __main__ - Step 303 : {'lr': 4.800000000000001e-06, 'samples': 606, 'steps': 18, 'loss/train': 7.9121599197387695, 'Epoch': 9}\n",
      "01/14/2023 18:09:31 - INFO - __main__ - Step 304 : {'lr': 4.800000000000001e-06, 'samples': 608, 'steps': 18, 'loss/train': 7.568471908569336, 'Epoch': 9}\n",
      "01/14/2023 18:09:31 - INFO - __main__ - Step 305 : {'lr': 5.066666666666667e-06, 'samples': 610, 'steps': 19, 'loss/train': 6.983767032623291, 'Epoch': 9}\n",
      "01/14/2023 18:09:32 - INFO - __main__ - Step 306 : {'lr': 5.066666666666667e-06, 'samples': 612, 'steps': 19, 'loss/train': 6.8040642738342285, 'Epoch': 9}\n",
      "01/14/2023 18:09:32 - INFO - __main__ - Step 307 : {'lr': 5.066666666666667e-06, 'samples': 614, 'steps': 19, 'loss/train': 7.108768463134766, 'Epoch': 9}\n",
      "01/14/2023 18:09:32 - INFO - __main__ - Step 308 : {'lr': 5.066666666666667e-06, 'samples': 616, 'steps': 19, 'loss/train': 7.295741081237793, 'Epoch': 9}\n",
      "01/14/2023 18:09:33 - INFO - __main__ - Step 309 : {'lr': 5.066666666666667e-06, 'samples': 618, 'steps': 19, 'loss/train': 7.225511074066162, 'Epoch': 9}\n",
      "01/14/2023 18:09:33 - INFO - __main__ - Step 310 : {'lr': 5.066666666666667e-06, 'samples': 620, 'steps': 19, 'loss/train': 7.507200717926025, 'Epoch': 9}\n",
      "01/14/2023 18:09:34 - INFO - __main__ - Step 311 : {'lr': 5.066666666666667e-06, 'samples': 622, 'steps': 19, 'loss/train': 7.418980121612549, 'Epoch': 9}\n",
      "01/14/2023 18:09:34 - INFO - __main__ - Step 312 : {'lr': 5.066666666666667e-06, 'samples': 624, 'steps': 19, 'loss/train': 7.2580695152282715, 'Epoch': 9}\n",
      "01/14/2023 18:09:34 - INFO - __main__ - Step 313 : {'lr': 5.066666666666667e-06, 'samples': 626, 'steps': 19, 'loss/train': 7.751252174377441, 'Epoch': 9}\n",
      "01/14/2023 18:09:35 - INFO - __main__ - Step 314 : {'lr': 5.066666666666667e-06, 'samples': 628, 'steps': 19, 'loss/train': 8.052531242370605, 'Epoch': 9}\n",
      "01/14/2023 18:09:35 - INFO - __main__ - Step 315 : {'lr': 5.066666666666667e-06, 'samples': 630, 'steps': 19, 'loss/train': 7.777552127838135, 'Epoch': 9}\n",
      "01/14/2023 18:09:35 - INFO - __main__ - Step 316 : {'lr': 5.066666666666667e-06, 'samples': 632, 'steps': 19, 'loss/train': 7.557708740234375, 'Epoch': 9}\n",
      "01/14/2023 18:09:36 - INFO - __main__ - Step 317 : {'lr': 5.066666666666667e-06, 'samples': 634, 'steps': 19, 'loss/train': 6.969289779663086, 'Epoch': 9}\n",
      "01/14/2023 18:09:36 - INFO - __main__ - Step 318 : {'lr': 5.066666666666667e-06, 'samples': 636, 'steps': 19, 'loss/train': 7.214503288269043, 'Epoch': 9}\n",
      "01/14/2023 18:09:37 - INFO - __main__ - Step 319 : {'lr': 5.066666666666667e-06, 'samples': 638, 'steps': 19, 'loss/train': 7.376826763153076, 'Epoch': 9}\n",
      "01/14/2023 18:09:37 - INFO - __main__ - Step 320 : {'lr': 5.066666666666667e-06, 'samples': 640, 'steps': 19, 'loss/train': 7.657264232635498, 'Epoch': 9}\n",
      "01/14/2023 18:09:37 - INFO - __main__ - Evaluating and Saving model after training\n",
      "01/14/2023 18:09:49 - INFO - __main__ - Step 321 : {'loss/eval': 7.880406856536865, 'perplexity': 2644.948486328125, 'Epoch': 9}\n",
      "Configuration saved in Training_files/models/codeparrot-small2/config.json\n",
      "Model weights saved in Training_files/models/codeparrot-small2/pytorch_model.bin\n",
      "Configuration saved in /tmp/tmp6_xocav1/config.json\n",
      "Model weights saved in /tmp/tmp6_xocav1/pytorch_model.bin\n",
      "Uploading the following files to susnato/codeparrot-small2: config.json,pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################\n",
      "                                    Starting Epoch - 10\n",
      "            ####################################################################################\n",
      "                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/14/2023 18:10:59 - INFO - __main__ - Step 1 : {'lr': 5.333333333333334e-06, 'samples': 2, 'steps': 0, 'loss/train': 7.29647970199585, 'Epoch': 10}\n",
      "01/14/2023 18:10:59 - INFO - __main__ - Step 2 : {'lr': 5.333333333333334e-06, 'samples': 4, 'steps': 0, 'loss/train': 7.377992630004883, 'Epoch': 10}\n",
      "01/14/2023 18:11:00 - INFO - __main__ - Step 3 : {'lr': 5.333333333333334e-06, 'samples': 6, 'steps': 0, 'loss/train': 7.587770462036133, 'Epoch': 10}\n",
      "01/14/2023 18:11:00 - INFO - __main__ - Step 4 : {'lr': 5.333333333333334e-06, 'samples': 8, 'steps': 0, 'loss/train': 7.324524402618408, 'Epoch': 10}\n",
      "01/14/2023 18:11:00 - INFO - __main__ - Step 5 : {'lr': 5.333333333333334e-06, 'samples': 10, 'steps': 0, 'loss/train': 7.182759761810303, 'Epoch': 10}\n",
      "01/14/2023 18:11:01 - INFO - __main__ - Step 6 : {'lr': 5.333333333333334e-06, 'samples': 12, 'steps': 0, 'loss/train': 7.5177812576293945, 'Epoch': 10}\n",
      "01/14/2023 18:11:01 - INFO - __main__ - Step 7 : {'lr': 5.333333333333334e-06, 'samples': 14, 'steps': 0, 'loss/train': 7.918083190917969, 'Epoch': 10}\n",
      "01/14/2023 18:11:01 - INFO - __main__ - Step 8 : {'lr': 5.333333333333334e-06, 'samples': 16, 'steps': 0, 'loss/train': 7.716144561767578, 'Epoch': 10}\n",
      "01/14/2023 18:11:02 - INFO - __main__ - Step 9 : {'lr': 5.333333333333334e-06, 'samples': 18, 'steps': 0, 'loss/train': 6.990382671356201, 'Epoch': 10}\n",
      "01/14/2023 18:11:02 - INFO - __main__ - Step 10 : {'lr': 5.333333333333334e-06, 'samples': 20, 'steps': 0, 'loss/train': 6.85659646987915, 'Epoch': 10}\n",
      "01/14/2023 18:11:02 - INFO - __main__ - Step 11 : {'lr': 5.333333333333334e-06, 'samples': 22, 'steps': 0, 'loss/train': 6.90040397644043, 'Epoch': 10}\n",
      "01/14/2023 18:11:03 - INFO - __main__ - Step 12 : {'lr': 5.333333333333334e-06, 'samples': 24, 'steps': 0, 'loss/train': 7.543766498565674, 'Epoch': 10}\n",
      "01/14/2023 18:11:03 - INFO - __main__ - Step 13 : {'lr': 5.333333333333334e-06, 'samples': 26, 'steps': 0, 'loss/train': 7.407714366912842, 'Epoch': 10}\n",
      "01/14/2023 18:11:03 - INFO - __main__ - Step 14 : {'lr': 5.333333333333334e-06, 'samples': 28, 'steps': 0, 'loss/train': 7.643468856811523, 'Epoch': 10}\n",
      "01/14/2023 18:11:04 - INFO - __main__ - Step 15 : {'lr': 5.333333333333334e-06, 'samples': 30, 'steps': 0, 'loss/train': 7.89756441116333, 'Epoch': 10}\n",
      "01/14/2023 18:11:04 - INFO - __main__ - Step 16 : {'lr': 5.333333333333334e-06, 'samples': 32, 'steps': 0, 'loss/train': 7.299304962158203, 'Epoch': 10}\n",
      "01/14/2023 18:11:05 - INFO - __main__ - Step 17 : {'lr': 5.600000000000001e-06, 'samples': 34, 'steps': 1, 'loss/train': 7.27489709854126, 'Epoch': 10}\n",
      "01/14/2023 18:11:05 - INFO - __main__ - Step 18 : {'lr': 5.600000000000001e-06, 'samples': 36, 'steps': 1, 'loss/train': 6.512881278991699, 'Epoch': 10}\n",
      "01/14/2023 18:11:05 - INFO - __main__ - Step 19 : {'lr': 5.600000000000001e-06, 'samples': 38, 'steps': 1, 'loss/train': 5.760300159454346, 'Epoch': 10}\n",
      "01/14/2023 18:11:06 - INFO - __main__ - Step 20 : {'lr': 5.600000000000001e-06, 'samples': 40, 'steps': 1, 'loss/train': 6.838194847106934, 'Epoch': 10}\n",
      "01/14/2023 18:11:06 - INFO - __main__ - Step 21 : {'lr': 5.600000000000001e-06, 'samples': 42, 'steps': 1, 'loss/train': 7.305314064025879, 'Epoch': 10}\n",
      "01/14/2023 18:11:06 - INFO - __main__ - Step 22 : {'lr': 5.600000000000001e-06, 'samples': 44, 'steps': 1, 'loss/train': 7.745231628417969, 'Epoch': 10}\n",
      "01/14/2023 18:11:07 - INFO - __main__ - Step 23 : {'lr': 5.600000000000001e-06, 'samples': 46, 'steps': 1, 'loss/train': 7.702905654907227, 'Epoch': 10}\n",
      "01/14/2023 18:11:07 - INFO - __main__ - Step 24 : {'lr': 5.600000000000001e-06, 'samples': 48, 'steps': 1, 'loss/train': 8.015229225158691, 'Epoch': 10}\n",
      "01/14/2023 18:11:07 - INFO - __main__ - Step 25 : {'lr': 5.600000000000001e-06, 'samples': 50, 'steps': 1, 'loss/train': 8.028060913085938, 'Epoch': 10}\n",
      "01/14/2023 18:11:08 - INFO - __main__ - Step 26 : {'lr': 5.600000000000001e-06, 'samples': 52, 'steps': 1, 'loss/train': 8.044950485229492, 'Epoch': 10}\n",
      "01/14/2023 18:11:08 - INFO - __main__ - Step 27 : {'lr': 5.600000000000001e-06, 'samples': 54, 'steps': 1, 'loss/train': 7.838504314422607, 'Epoch': 10}\n",
      "01/14/2023 18:11:09 - INFO - __main__ - Step 28 : {'lr': 5.600000000000001e-06, 'samples': 56, 'steps': 1, 'loss/train': 7.720620155334473, 'Epoch': 10}\n",
      "01/14/2023 18:11:09 - INFO - __main__ - Step 29 : {'lr': 5.600000000000001e-06, 'samples': 58, 'steps': 1, 'loss/train': 7.3168864250183105, 'Epoch': 10}\n",
      "01/14/2023 18:11:09 - INFO - __main__ - Step 30 : {'lr': 5.600000000000001e-06, 'samples': 60, 'steps': 1, 'loss/train': 5.571746349334717, 'Epoch': 10}\n",
      "01/14/2023 18:11:10 - INFO - __main__ - Step 31 : {'lr': 5.600000000000001e-06, 'samples': 62, 'steps': 1, 'loss/train': 5.368271350860596, 'Epoch': 10}\n",
      "01/14/2023 18:11:10 - INFO - __main__ - Step 32 : {'lr': 5.600000000000001e-06, 'samples': 64, 'steps': 1, 'loss/train': 6.659700393676758, 'Epoch': 10}\n",
      "01/14/2023 18:11:10 - INFO - __main__ - Step 33 : {'lr': 5.866666666666667e-06, 'samples': 66, 'steps': 2, 'loss/train': 8.049028396606445, 'Epoch': 10}\n",
      "01/14/2023 18:11:11 - INFO - __main__ - Step 34 : {'lr': 5.866666666666667e-06, 'samples': 68, 'steps': 2, 'loss/train': 7.28830623626709, 'Epoch': 10}\n",
      "01/14/2023 18:11:11 - INFO - __main__ - Step 35 : {'lr': 5.866666666666667e-06, 'samples': 70, 'steps': 2, 'loss/train': 7.581616401672363, 'Epoch': 10}\n",
      "01/14/2023 18:11:11 - INFO - __main__ - Step 36 : {'lr': 5.866666666666667e-06, 'samples': 72, 'steps': 2, 'loss/train': 6.703240871429443, 'Epoch': 10}\n",
      "01/14/2023 18:11:12 - INFO - __main__ - Step 37 : {'lr': 5.866666666666667e-06, 'samples': 74, 'steps': 2, 'loss/train': 7.919386863708496, 'Epoch': 10}\n",
      "01/14/2023 18:11:12 - INFO - __main__ - Step 38 : {'lr': 5.866666666666667e-06, 'samples': 76, 'steps': 2, 'loss/train': 8.098630905151367, 'Epoch': 10}\n",
      "01/14/2023 18:11:12 - INFO - __main__ - Step 39 : {'lr': 5.866666666666667e-06, 'samples': 78, 'steps': 2, 'loss/train': 6.897085666656494, 'Epoch': 10}\n",
      "01/14/2023 18:11:13 - INFO - __main__ - Step 40 : {'lr': 5.866666666666667e-06, 'samples': 80, 'steps': 2, 'loss/train': 6.476614475250244, 'Epoch': 10}\n",
      "01/14/2023 18:11:13 - INFO - __main__ - Step 41 : {'lr': 5.866666666666667e-06, 'samples': 82, 'steps': 2, 'loss/train': 6.480367183685303, 'Epoch': 10}\n",
      "01/14/2023 18:11:14 - INFO - __main__ - Step 42 : {'lr': 5.866666666666667e-06, 'samples': 84, 'steps': 2, 'loss/train': 8.116950035095215, 'Epoch': 10}\n",
      "01/14/2023 18:11:14 - INFO - __main__ - Step 43 : {'lr': 5.866666666666667e-06, 'samples': 86, 'steps': 2, 'loss/train': 8.368298530578613, 'Epoch': 10}\n",
      "01/14/2023 18:11:14 - INFO - __main__ - Step 44 : {'lr': 5.866666666666667e-06, 'samples': 88, 'steps': 2, 'loss/train': 8.048543930053711, 'Epoch': 10}\n",
      "01/14/2023 18:11:15 - INFO - __main__ - Step 45 : {'lr': 5.866666666666667e-06, 'samples': 90, 'steps': 2, 'loss/train': 7.242039680480957, 'Epoch': 10}\n",
      "01/14/2023 18:11:15 - INFO - __main__ - Step 46 : {'lr': 5.866666666666667e-06, 'samples': 92, 'steps': 2, 'loss/train': 7.223912715911865, 'Epoch': 10}\n",
      "01/14/2023 18:11:15 - INFO - __main__ - Step 47 : {'lr': 5.866666666666667e-06, 'samples': 94, 'steps': 2, 'loss/train': 7.741973876953125, 'Epoch': 10}\n",
      "01/14/2023 18:11:16 - INFO - __main__ - Step 48 : {'lr': 5.866666666666667e-06, 'samples': 96, 'steps': 2, 'loss/train': 7.269099712371826, 'Epoch': 10}\n",
      "01/14/2023 18:11:16 - INFO - __main__ - Step 49 : {'lr': 6.133333333333334e-06, 'samples': 98, 'steps': 3, 'loss/train': 7.506803035736084, 'Epoch': 10}\n",
      "01/14/2023 18:11:16 - INFO - __main__ - Step 50 : {'lr': 6.133333333333334e-06, 'samples': 100, 'steps': 3, 'loss/train': 7.0218119621276855, 'Epoch': 10}\n",
      "01/14/2023 18:11:17 - INFO - __main__ - Step 51 : {'lr': 6.133333333333334e-06, 'samples': 102, 'steps': 3, 'loss/train': 7.314415454864502, 'Epoch': 10}\n",
      "01/14/2023 18:11:17 - INFO - __main__ - Step 52 : {'lr': 6.133333333333334e-06, 'samples': 104, 'steps': 3, 'loss/train': 7.183579921722412, 'Epoch': 10}\n",
      "01/14/2023 18:11:18 - INFO - __main__ - Step 53 : {'lr': 6.133333333333334e-06, 'samples': 106, 'steps': 3, 'loss/train': 7.552597522735596, 'Epoch': 10}\n",
      "01/14/2023 18:11:18 - INFO - __main__ - Step 54 : {'lr': 6.133333333333334e-06, 'samples': 108, 'steps': 3, 'loss/train': 7.698977470397949, 'Epoch': 10}\n",
      "01/14/2023 18:11:18 - INFO - __main__ - Step 55 : {'lr': 6.133333333333334e-06, 'samples': 110, 'steps': 3, 'loss/train': 7.245739936828613, 'Epoch': 10}\n",
      "01/14/2023 18:11:19 - INFO - __main__ - Step 56 : {'lr': 6.133333333333334e-06, 'samples': 112, 'steps': 3, 'loss/train': 7.5772552490234375, 'Epoch': 10}\n",
      "01/14/2023 18:11:19 - INFO - __main__ - Step 57 : {'lr': 6.133333333333334e-06, 'samples': 114, 'steps': 3, 'loss/train': 7.506058692932129, 'Epoch': 10}\n",
      "01/14/2023 18:11:19 - INFO - __main__ - Step 58 : {'lr': 6.133333333333334e-06, 'samples': 116, 'steps': 3, 'loss/train': 7.642079830169678, 'Epoch': 10}\n",
      "01/14/2023 18:11:20 - INFO - __main__ - Step 59 : {'lr': 6.133333333333334e-06, 'samples': 118, 'steps': 3, 'loss/train': 8.6851224899292, 'Epoch': 10}\n",
      "01/14/2023 18:11:20 - INFO - __main__ - Step 60 : {'lr': 6.133333333333334e-06, 'samples': 120, 'steps': 3, 'loss/train': 7.5473833084106445, 'Epoch': 10}\n",
      "01/14/2023 18:11:20 - INFO - __main__ - Step 61 : {'lr': 6.133333333333334e-06, 'samples': 122, 'steps': 3, 'loss/train': 7.337690353393555, 'Epoch': 10}\n",
      "01/14/2023 18:11:21 - INFO - __main__ - Step 62 : {'lr': 6.133333333333334e-06, 'samples': 124, 'steps': 3, 'loss/train': 6.60353422164917, 'Epoch': 10}\n",
      "01/14/2023 18:11:21 - INFO - __main__ - Step 63 : {'lr': 6.133333333333334e-06, 'samples': 126, 'steps': 3, 'loss/train': 6.8683929443359375, 'Epoch': 10}\n",
      "01/14/2023 18:11:22 - INFO - __main__ - Step 64 : {'lr': 6.133333333333334e-06, 'samples': 128, 'steps': 3, 'loss/train': 6.951753616333008, 'Epoch': 10}\n",
      "01/14/2023 18:11:22 - INFO - __main__ - Step 65 : {'lr': 6.4000000000000006e-06, 'samples': 130, 'steps': 4, 'loss/train': 7.439599514007568, 'Epoch': 10}\n",
      "01/14/2023 18:11:22 - INFO - __main__ - Step 66 : {'lr': 6.4000000000000006e-06, 'samples': 132, 'steps': 4, 'loss/train': 7.282679080963135, 'Epoch': 10}\n",
      "01/14/2023 18:11:23 - INFO - __main__ - Step 67 : {'lr': 6.4000000000000006e-06, 'samples': 134, 'steps': 4, 'loss/train': 7.121038913726807, 'Epoch': 10}\n",
      "01/14/2023 18:11:23 - INFO - __main__ - Step 68 : {'lr': 6.4000000000000006e-06, 'samples': 136, 'steps': 4, 'loss/train': 7.289590358734131, 'Epoch': 10}\n",
      "01/14/2023 18:11:23 - INFO - __main__ - Step 69 : {'lr': 6.4000000000000006e-06, 'samples': 138, 'steps': 4, 'loss/train': 8.218487739562988, 'Epoch': 10}\n",
      "01/14/2023 18:11:24 - INFO - __main__ - Step 70 : {'lr': 6.4000000000000006e-06, 'samples': 140, 'steps': 4, 'loss/train': 7.738712787628174, 'Epoch': 10}\n",
      "01/14/2023 18:11:24 - INFO - __main__ - Step 71 : {'lr': 6.4000000000000006e-06, 'samples': 142, 'steps': 4, 'loss/train': 7.785006046295166, 'Epoch': 10}\n",
      "01/14/2023 18:11:24 - INFO - __main__ - Step 72 : {'lr': 6.4000000000000006e-06, 'samples': 144, 'steps': 4, 'loss/train': 7.770621299743652, 'Epoch': 10}\n",
      "01/14/2023 18:11:25 - INFO - __main__ - Step 73 : {'lr': 6.4000000000000006e-06, 'samples': 146, 'steps': 4, 'loss/train': 7.336471080780029, 'Epoch': 10}\n",
      "01/14/2023 18:11:25 - INFO - __main__ - Step 74 : {'lr': 6.4000000000000006e-06, 'samples': 148, 'steps': 4, 'loss/train': 7.57607889175415, 'Epoch': 10}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrun_epochs\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 36\u001B[0m, in \u001B[0;36mrun_epochs\u001B[0;34m(epochs)\u001B[0m\n\u001B[1;32m     33\u001B[0m log_metrics(epoch, step, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m:get_lr(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msamples\u001B[39m\u001B[38;5;124m\"\u001B[39m:step\u001B[38;5;241m*\u001B[39msamples_per_step,\n\u001B[1;32m     34\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msteps\u001B[39m\u001B[38;5;124m\"\u001B[39m:completed_steps, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss/train\u001B[39m\u001B[38;5;124m\"\u001B[39m:loss\u001B[38;5;241m.\u001B[39mitem()})\n\u001B[1;32m     35\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m/\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n\u001B[0;32m---> 36\u001B[0m \u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m step \u001B[38;5;241m%\u001B[39m args\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     38\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.9/site-packages/accelerate/accelerator.py:1316\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[0;34m(self, loss, **kwargs)\u001B[0m\n\u001B[1;32m   1314\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler\u001B[38;5;241m.\u001B[39mscale(loss)\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1316\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.9/site-packages/torch/_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    487\u001B[0m     )\n\u001B[0;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "run_epochs(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run_epochs(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
